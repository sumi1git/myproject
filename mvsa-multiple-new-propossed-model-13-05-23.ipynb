{"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"                                                                                                                                                                                        # This Python 3 environment comes with many helpful analytics libraries installed\n#import libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport matplotlib.pyplot as plt\n","metadata":{"id":"fgFPxlL52Zvs","execution":{"iopub.status.busy":"2023-06-05T08:50:34.373916Z","iopub.execute_input":"2023-06-05T08:50:34.374774Z","iopub.status.idle":"2023-06-05T08:50:34.385584Z","shell.execute_reply.started":"2023-06-05T08:50:34.374736Z","shell.execute_reply":"2023-06-05T08:50:34.384735Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"gpu_info = !nvidia-smi\ngpu_info = '\\n'.join(gpu_info)\nif gpu_info.find('failed') >= 0:\n  print('Not connected to a GPU')\nelse:\n  print(gpu_info)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qHOItF0cypmL","outputId":"d1377373-f419-43e0-e264-c4eea7e3ef1c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from psutil import virtual_memory\nram_gb = virtual_memory().total / 1e9\nprint('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n\nif ram_gb < 20:\n  print('Not using a high-RAM runtime')\nelse:\n  print('You are using a high-RAM runtime!')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5QanE58dysi8","outputId":"8f2ee655-3e7e-4456-f1b3-a6683b8a19e4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"rgEEUg7o2xZJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"id":"xhvrpI4D2Zvw"}},{"cell_type":"markdown","source":"****","metadata":{"id":"gGUIDg-c2Zvy"}},{"cell_type":"markdown","source":"****DATA CLeaning****","metadata":{"id":"HQqLvXIB2Zvz"}},{"cell_type":"code","source":"#load csv files\n#ddf=pd.read_csv('/kaggle/input/mvsa-multiple-attcsv/mvsa_multiple_attr.csv')\n#df=pd.read_csv('/kaggle/input/simpson-attrbutes/simpson_attr_info.csv')\n#df=pd.read_csv('/gdrive/MyDrive/sumana_nuig/mvsa_multiple/msva-multiple_jointlabel.csv')\ndf=pd.read_csv('/kaggle/input/mvsa-multiple-obj-att-clean/mvsa_multiple_attr_clean.csv')\n#df1=pd.read_csv('/gdrive/MyDrive/sumana_nuig/mvsa/mvsa_attr_info1.csv')\ndf=df.dropna()\n\n#df1=df1.iloc[:100:,]\ndf.shape\n#df[:2]\n#df.joint_sentiment.value_counts()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s93iLx4I2Zvz","outputId":"cc073d01-14d0-42cf-945a-b935dae471a4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get batch of 64 we need 2844 sample\n#df=df.iloc[:4288:,]","metadata":{"id":"u5zmPeu92Zv1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\ndf[\"text\"] = df[\"text\"].apply(lambda x: re.sub(\"[\\[{','}\\]]\", ' ', str(x)).strip())#captin text\ndf[\"class_att1\"] = df[\"class_att\"].apply(lambda x: re.sub(\"[\\[{','}\\]]\", ' ', str(x)).strip())#class-attributes pair\ndf[\"class_names\"] = df[\"object_names\"].apply(lambda x: re.sub(\"[\\[{','}\\]]\", ' ', str(x)).strip())#class names\ndf[\"cbt_class_attribute\"]=df[\"text\"].str.cat(df[\"class_att1\"])#Combined caption and class-attributes pairs\ndf[\"cbt_class\"]=df[\"text\"].str.cat(df[\"class_names\"])#Combined caption and class-attributes pairs","metadata":{"id":"E8lVkQXUpWyg","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[:2]","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":367},"id":"hkE0d34mKNoW","outputId":"40d3cafd-a923-483f-bbda-61568faf6ec9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Balancing data in the dataframe**","metadata":{"id":"LWJ32YNw2Zv3"}},{"cell_type":"code","source":"GLOVE_EMB = '/kaggle/input/glove6b300d/glove.6B.300d.txt'\nEMBEDDING_DIM = 300","metadata":{"id":"xY-8piHO2Zv3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training and Test\ndef load_tarin_test_dataset(df,col,str_len):\n    column=col\n    max_len=str_len\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts( df[column])\n    #tokenizer.fit_on_texts(train_df.object_attr1_text)\n    #tokenizer.fit_on_texts(train_df.text)\n    #tokenizer1.fit_on_texts(test_data)\n\n    word_index = tokenizer.word_index\n    vocab_size = len(tokenizer.word_index) + 1\n    #print(\"Vocabulary Size :\", vocab_size)\n\n    #jx_train = pad_sequences(tokenizer.texts_to_sequences(train_df.text), maxlen = max_len)\n    x_df = pad_sequences(tokenizer.texts_to_sequences( df[column]), maxlen = max_len)\n    #x_val = pad_sequences(tokenizer.texts_to_sequences(val_df.class_att), maxlen = 32)\n    #jx_test = pad_sequences(tokenizer.texts_to_sequences(test_df.text), maxlen =max_len )\n    #x_test = pad_sequences(tokenizer.texts_to_sequences( test[column]), maxlen =max_len )\n    #y_train, y_test = train_test_split(y, test_size=1-TRAIN_SIZE,random_state=7) # Splits Dataset into Training and Testing set \n    #y_train, y_test = train_test_split(df['POSITIVE'], test_size=1-TRAIN_SIZE,random_state=7)\n    embeddings_index = {}\n\n    f = open(GLOVE_EMB, encoding=\"utf8\")\n    for line in f :\n        values = line.split()\n        word = value = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n    f.close()\n\n    print('Found %s word vectors.' %len(embeddings_index))\n    embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n    print(embedding_matrix.shape)\n    for word, i in word_index.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n    \n    embedding_layer = tf.keras.layers.Embedding(vocab_size,\n                                          EMBEDDING_DIM,\n                                          weights=[embedding_matrix],\n                                          input_length=max_len,#MAX_SEQUENCE_LENGTH,\n                                          trainable=False)\n    return x_df,vocab_size,embedding_layer","metadata":{"id":"-6ZwPxESptoP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install keras_preprocessing","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cA3_KcHQ2Zv9","outputId":"f4f78091-1fee-40f2-9c82-d6a4add83f1f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Glob tokenizer\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.optimizers import Adam\nfrom keras_preprocessing.text import Tokenizer\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\nfrom keras_preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Conv1D, Conv2D,MaxPooling2D,Bidirectional, LSTM, Dense, Input, Flatten,Dropout,GlobalMaxPool1D,MaxPooling1D,Concatenate\nfrom tensorflow.keras.layers import SpatialDropout1D","metadata":{"id":"fG16yEf12Zv9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#module for extracting text features\ndef cnn_model_for_text(num_filters,kernel_size,tx_df,embeding_layer):\n    \n    num_filters=num_filters\n    kernel_size=kernel_size\n           \n    #jsequence_input = Input(shape=(maxlen,), dtype='int32')\n    #sequence_input = Input(shape=(max_len,), dtype='int32')\n    #embedding_sequences = embedding_layer(sequence_input)\n    embedding_sequences = embedding_layer(tx_df)\n    x = SpatialDropout1D(0.2)(embedding_sequences)\n    conv_layers = []\n    for num_filters,kernel_size in zip(num_filters,kernel_size):\n        conv_layer = Conv1D(filters=num_filters, kernel_size=kernel_size, activation='relu')(x)#(jembedding_sequences)\n        pool_layer = MaxPooling1D(pool_size=max_len - kernel_size + 1)(conv_layer)\n        #pool_layer = MaxPooling1D()(conv_layer)\n        #pool_layer = tf.keras.layers.Reshape((1, -1))(pool_layer)\n        conv_layers.append(pool_layer)\n\n    #Concatenate all convolutional layers\n    concatenate_layer = tf.keras.layers.Concatenate()(conv_layers)\n    flatten_layer = Flatten()(concatenate_layer)\n    x = Dropout(0.5)(flatten_layer)#(x)\n    #txt = Dense(256, activation='tanh')(x)\n    txt = Dense(256, activation='relu')(x)\n    \n    return txt","metadata":{"id":"dGh-OyDhidQF","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#class attributes features\nmax_len=max([len(x.split()) for x in df.class_att1])\n#tx_train,tx_test,vocab_size,embedding_layer=load_tarin_test_dataset(train_df,test_df,'text',max([len(x.split()) for x in train_df.text]))\nclassatt_df,vocab_size,embedding_layer=load_tarin_test_dataset(df,'class_att1',max_len)\nprint(\"Vocabulary Size :\", vocab_size)\nprint(\"Training X Shape:\",classatt_df.shape)\n#print(\"Testing X Shape:\",jx_test.shape)\n#call cnn\nkernel_sizes = [2,3,4]\nnum_filters = [8,16,32]\nclass_attributes_features=cnn_model_for_text(num_filters,kernel_sizes,classatt_df,embedding_layer)\nprint((np.array(class_attributes_features)).shape)\n","metadata":{"id":"MOwZ5UM5na55","colab":{"base_uri":"https://localhost:8080/"},"outputId":"de7c4082-7074-4cb6-ead1-ccd789c171e3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bilstm_model(tx_df,embeding_layer):\n    \n    embedding_sequences = embedding_layer(tx_df)\n    #embedding_sequences = embedding_layer(sequence_input)\n    txt = SpatialDropout1D(0.1)(embedding_sequences)\n    #txt=Conv1D(100, 4,activation='relu')(txt)#(tembedding_sequences)#(x)\n    txt = Bidirectional(LSTM(32, dropout=0.2, recurrent_dropout=0.2))(txt)#(tembedding_sequences)\n    #ttxtb = Dense(256, activation='tanh')(txtb)\n    txt = Dense(256, activation='relu')(txt)\n    return txt","metadata":{"id":"wAz47a6vnbMf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#combined-text-features with class-attributes collection\nmax_len=max([len(x.split()) for x in df.cbt_class_attribute])\n#tx_train,tx_test,vocab_size,embedding_layer=load_tarin_test_dataset(train_df,test_df,'text',max([len(x.split()) for x in train_df.text]))\ncombined_text_att,vocab_size,embedding_layer=load_tarin_test_dataset(df,'cbt_class_attribute',max_len)\nprint(\"Vocabulary Size :\", vocab_size)\nprint(\"Training X Shape:\",combined_text_att.shape)\n#print(\"Testing X Shape:\",jx_test.shape)\n#call bilstm\n\ncombined_text_att_features=bilstm_model(combined_text_att,embedding_layer)\nprint((np.array(combined_text_att_features)).shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9B-RomO5nbgs","outputId":"2c472689-a37f-4b9a-a563-c429a6a0d9c3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#caption text features collection\nmax_len=max([len(x.split()) for x in df.text])\n#tx_train,tx_test,vocab_size,embedding_layer=load_tarin_test_dataset(train_df,test_df,'text',max([len(x.split()) for x in train_df.text]))\ntx_df,vocab_size,embedding_layer=load_tarin_test_dataset(df,'text',max_len)\nprint(\"Vocabulary Size :\", vocab_size)\nprint(\"Training X Shape:\",tx_df.shape)\n#print(\"Testing X Shape:\",tx_test.shape)\n\n#call bilstm model\nkernel_sizes = [2,3,4]\nnum_filters = [8,16,32]\ncnn_text_features=cnn_model_for_text(num_filters,kernel_sizes,tx_df,embedding_layer)\nprint((np.array(cnn_text_features)).shape)\n\nbilstm_text_features=bilstm_model(tx_df,embedding_layer)\nprint((np.array(bilstm_text_features)).shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xf8nCpEDsnss","outputId":"66cfc9e0-c683-4c89-e561-d7491781642f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#combined-text-features with class-attributes collection\nmax_len=max([len(x.split()) for x in df.cbt_class_attribute])\n#tx_train,tx_test,vocab_size,embedding_layer=load_tarin_test_dataset(train_df,test_df,'text',max([len(x.split()) for x in train_df.text]))\ncombined_text_att,vocab_size,embedding_layer=load_tarin_test_dataset(df,'cbt_class_attribute',max_len)\nprint(\"Vocabulary Size :\", vocab_size)\nprint(\"Training X Shape:\",combined_text_att.shape)\n#print(\"Testing X Shape:\",jx_test.shape)\n#call cnn\nkernel_sizes = [3,4,5]\nnum_filters = [16,32,64]\ncbt_class_attributes_features=bilstm_model(combined_text_att,embedding_layer)\nprint((np.array(cbt_class_attributes_features)).shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Qjg1J2A3UaP","outputId":"5d2cf87f-db5f-4927-dd4b-f8bf1f6a6137"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"id":"6o1AowbK2ZwG"}},{"cell_type":"markdown","source":"**Visual Features Extraction**","metadata":{"id":"YJIHjhIc2ZwI"}},{"cell_type":"code","source":"from tensorflow.keras.applications import VGG16,VGG19,ResNet50\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom keras.layers import Input,Dense, Dropout, Flatten,BatchNormalization,concatenate,Bidirectional,LSTM,GlobalAveragePooling2D\nfrom keras.layers import BatchNormalization\nimport matplotlib.pyplot as plt","metadata":{"id":"RNsNE7Y_2ZwJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Object visual featutes**","metadata":{"id":"fn6Jl5h02ZwK"}},{"cell_type":"code","source":"\"\"\"\nnp.savez('/gdrive/MyDrive/sumana_nuig/mvsa/mvsa_single_allfeatures.npz',\n            img_file=np.array(img_file),\n            global_features=np.array(global_features),\n            local_object_features=np.array(local_object_features),\n            text_features=np.array(text_features),\n            class_features=np.array(class_features),\n            class_attributes_features=np.array(class_attributes_features),\n            cbt_class_features=np.array(cbt_class_features),\n            cbt_class_attributes_features=np.array(cbt_class_attributes_features),\n            target_labels=np.array(target_labels))\n\"\"\"","metadata":{"id":"WJSsCMH-99Tg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data = np.load('/gdrive/MyDrive/sumana_nuig/mvsa/mvsa_single_allfeatures.npz')\ndata = np.load('/kaggle/input/mvsa-multiple-allfeatures/mvsa_multiple_allfeatures.npz')\n\n# Access the variables inside the npz file\nimg_file = data['img_file']\nglobal_features = data['global_features']\nlocal_object_features = data['local_object_features']\ntext_features=data['text_features']\nclass_features=data['class_features']\nclass_attributes_features=data['class_attributes_features']\ncbt_class_features=data['cbt_class_features']\ncbt_class_attributes_features=data['cbt_class_attributes_features']\ntarget_labels=data['target_labels']\nprint(global_features.shape,local_object_features.shape,target_labels.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6O7xxp3ddHxu","outputId":"9b12a22f-d18d-479c-862f-376622601f62","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_global_features.shape, train_objects_features.shape,local_object_features.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FnlyJkfyirJW","outputId":"0f2937bc-577c-4fe1-969c-7dc5a4ceba14"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nfrom sklearn.model_selection import train_test_split\n\n# Split global and object features\ntrain_global_features,test_global_features, train_objects_features, test_objects_features= train_test_split(global_features,\n                                                                                                            local_object_features,\n                                                                                                            test_size=0.1)\n# split class,class_attributes features\ntrain_text_features,test_text_features,train_class_features,test_class_features, train_class_attributes_features, test_class_attributes_features = train_test_split(\n                                                                                                                          text_features,\n                                                                                                                          class_features,\n                                                                                                                          class_attributes_features,\n                                                                                                                          test_size=0.1)\n# split cbt_class_text,cbt_class_attributes_text\ntrain_cbt_class_features,test_cbt_class_features,  train_cbt_class_attributes_features,test_cbt_class_attributes_features = train_test_split(\n                                                                                                                          class_features,\n                                                                                                                          class_attributes_features,\n                                                                                                                          test_size=0.1)\n# split target labels\ny_train,y_test = train_test_split(target_labels,test_size=0.1)\n\"\"\"\n\n                                                                                                                          ","metadata":{"id":"MrnKU-FjZCCL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"id":"VcVUn5TTzyYh"}},{"cell_type":"code","source":"#Process train data\n#Cross attention of object_features and class_features with batch size\nimport torch\nimport torch.nn as nn\nimport numpy as np\n#train_classat_features, rtrain_object_features, rtrain_object_features)\n#text=np.array(class_features)#class features\ntext=np.array(class_attributes_features) #class-attributes pair features\nprint(text.shape)\nimage=np.array(local_object_features)\n#image=image_features_arr\n#image=global_features\nprint(image.shape)\n# Convert numpy arrays to torch tensors\ntext_tensor = torch.from_numpy(text)\ntext_tensor=text_tensor.unsqueeze(2)\n#text_tensor=text_tensor.permute(0, 1)\nimage_tensor = torch.from_numpy(image)\nimage_tensor=image_tensor.unsqueeze(2)\n#image_tensor=image_tensor.permute(0, 1)\n\n# Define a cross-attention model\nclass CrossAttention(nn.Module):\n    def __init__(self, text_dim, image_dim):\n        super(CrossAttention, self).__init__()\n        self.text_dim = text_dim\n        self.image_dim = image_dim\n        #self.linear_text = nn.Linear(text_dim, image_dim)\n        #self.linear_image = nn.Linear(image_dim, text_dim)\n\n    def forward(self, text_tensor, image_tensor):\n        # Compute attention weights\n        \n        self.text_tensor = text_tensor\n        self.image_tensor = image_tensor\n\n        # Compute attended features\n    \n        attention_score = torch.bmm(image_tensor.transpose(1, 2), text_tensor)\n        \n        attention_score = torch.softmax(attention_score, dim=1)\n        #attention_score = attention_score.unsqueeze(2) # transpose back to [batch_size, sequence_length, feature_dim]\n        context = (torch.bmm(image_tensor,attention_score.transpose(1,2)))\n        #image_attended = torch.matmul(text_att.transpose(0, 1), image)\n        #image_attended = image_attended.transpose(0, 1)\n        print(context.shape)\n        print(image_tensor.shape)\n        # Concatenate attended features\n        attended_features = torch.cat([context, image_tensor], dim=1)\n\n        return attended_features\n\n# Instantiate the cross-attention model\ncross_att = CrossAttention(text_dim=text.shape[1], image_dim=image.shape[1])\n#cross_att = CrossAttention(text_dim=(text.shape), image_dim=(image.shape))\n\n# Perform cross-attention\nattended_features1 = cross_att(text_tensor.float(), image_tensor.float())\nattended_features = attended_features1.squeeze(2)\nprint(attended_features.shape)\n# Convert the attended features back to numpy array\nobject_class_features = attended_features.detach().numpy()\nprint(object_class_features.shape)\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7fUuns17fSOl","outputId":"cda1c9ec-a013-475e-99a6-f20b31df0c1c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#Cross attention of global features and text_features with batch size\nimport torch\nimport torch.nn as nn\nimport numpy as np\n#train_classat_features, rtrain_object_features, rtrain_object_features)\n#text=np.array(class_features)#class features\n#text=np.array(class_attributes_features) #class-attributes pair features\ntext=np.array(text_features) #class-attributes pair features\nprint(text.shape)\n#image=np.array(text_features)\nimage=np.array(global_features)\n#image=image_features_arr\n#image=global_features\nprint(image.shape)\n# Convert numpy arrays to torch tensors\ntext_tensor = torch.from_numpy(text)\ntext_tensor=text_tensor.unsqueeze(2)\n#text_tensor=text_tensor.permute(0, 1)\nimage_tensor = torch.from_numpy(image)\nimage_tensor=image_tensor.unsqueeze(2)\n#image_tensor=image_tensor.permute(0, 1)\n\n# Define a cross-attention model\nclass CrossAttention(nn.Module):\n    def __init__(self, text_dim, image_dim):\n        super(CrossAttention, self).__init__()\n        self.text_dim = text_dim\n        self.image_dim = image_dim\n        #self.linear_text = nn.Linear(text_dim, image_dim)\n        #self.linear_image = nn.Linear(image_dim, text_dim)\n\n    def forward(self, text_tensor, image_tensor):\n        # Compute attention weights\n        \n        self.text_tensor = text_tensor\n        self.image_tensor = image_tensor\n\n        # Compute attended features\n    \n        attention_score = torch.bmm(image_tensor.transpose(1, 2), text_tensor)\n        \n        attention_score = torch.softmax(attention_score, dim=1)\n        #attention_score = attention_score.unsqueeze(2) # transpose back to [batch_size, sequence_length, feature_dim]\n        context = (torch.bmm(image_tensor,attention_score.transpose(1,2)))\n        #image_attended = torch.matmul(text_att.transpose(0, 1), image)\n        #image_attended = image_attended.transpose(0, 1)\n        print(context.shape)\n        print(image_tensor.shape)\n        # Concatenate attended features\n        attended_features = torch.cat([context, image_tensor], dim=1)\n\n        return attended_features\n\n# Instantiate the cross-attention model\ncross_att = CrossAttention(text_dim=text.shape[1], image_dim=image.shape[1])\n#cross_att = CrossAttention(text_dim=(text.shape), image_dim=(image.shape))\n\n# Perform cross-attention\n#attended_features2 = cross_att(text_tensor.float(), image_tensor.float())\n#attended_features2 = attended_features2.squeeze(2)\n#print(attended_features2.shape)\n#cross attention with global image and text\nglobal_text_features1 = cross_att(text_tensor.float(), image_tensor.float())\nglobal_text_features = global_text_features1.squeeze(2)\nprint(global_text_features.shape)\n# Convert the attended features back to numpy array\nglobal_text_features = global_text_features.detach().numpy()\nprint(global_text_features.shape)\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_nTz1CrM-xIe","outputId":"8aec5d82-4516-4c82-8860-3856eb72a355","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Additive attention of caption text_features and class_features with batch size\nimport torch\nimport torch.nn as nn\nimport numpy as np\n#train_classat_features, rtrain_object_features, rtrain_object_features)\n#class_text=np.array(class_features)\nclass_text=np.array(class_attributes_features)\n#print(text.shape)\ncaption_text=np.array(text_features)\n#print(image.shape)\n# Convert numpy arrays to torch tensors\nclass_tensor = torch.from_numpy(class_text)\nclass_tensor=class_tensor.unsqueeze(2)\n#text_tensor=text_tensor.permute(0, 1)\ncaption_tensor = torch.from_numpy(caption_text)\ncaption_tensor=caption_tensor.unsqueeze(2)\n#image_tensor=image_tensor.permute(0, 1)\n\n# Define a cross-attention model\nclass AdditiveAttention(nn.Module):\n    def __init__(self, class_dim, caption_dim):\n        super(AdditiveAttention, self).__init__()\n        self.class_dim = class_dim\n        self.caption_dim = caption_dim\n        #self.linear_text = nn.Linear(text_dim, image_dim)\n        #self.linear_image = nn.Linear(image_dim, text_dim)\n\n    def forward(self, class_tensor, caption_tensor):\n        # Compute attention weights\n        \n        self.class_tensor = class_tensor\n        self.caption_tensor = caption_tensor\n\n        # Compute attended features\n        attention_score = class_tensor+caption_tensor\n        tanh_score = torch.tanh(attention_score)\n        print(tanh_score.shape)\n        attention_score = torch.softmax(tanh_score, dim=1)\n        print(attention_score.shape)\n        #attention_score = attention_score.unsqueeze(2) # transpose back to [batch_size, sequence_length, feature_dim]\n        context = (torch.bmm(attention_score,class_tensor.transpose(1,2)))\n        #image_attended = torch.matmul(text_att.transpose(0, 1), image)\n        #image_attended = image_attended.transpose(0, 1)\n        print(context.shape)\n        print(caption_tensor.shape)\n        #print(caption_tensor.transpose(1,2).shape)\n        # Concatenate attended features\n        #attended_features = torch.cat([context, caption_tensor.transpose(1,2)], dim=1)\n        attended_features = torch.cat([context[:, :, 0], caption_tensor[:, :, 0]], dim=1)\n\n        #attended_features = torch.cat([context, caption_tensor], dim=0)\n\n        return attended_features\n\n# Instantiate the cross-attention model\nadditive_att = AdditiveAttention(class_dim=class_text.shape[1], caption_dim=caption_text.shape[1])\n#cross_att = CAttention(text_dim=(text.shape), image_dim=(image.shape))\n\n# Perform cross-attention\nadditive_features_text_class = additive_att(class_tensor.float(), caption_tensor.float())\nadditive_features_text_class1 = additive_features_text_class.unsqueeze(2)\n#print(additive_attention2.shape)\n# Convert the attended features back to numpy array\n#additive_features_text_class1 = additive_features_text_class1.detach().numpy()\nprint(additive_features_text_class1.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tn8NTzYv8SqO","outputId":"10e4a491-0d3b-4f40-c62a-93e082ec9ac5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#2nd level cross attention of global_text_features1 and additive_features_text_class1\nimport torch\nimport torch.nn as nn\nimport numpy as np\n#train_classat_features, rtrain_object_features, rtrain_object_features)\n#text=np.array(class_features)#class features\ntext=np.array(class_attributes_features) #class-attributes pair features\nprint(text.shape)\nimage=np.array(text_features)\n#image=image_features_arr\n#image=global_features\nprint(image.shape)\n# Convert numpy arrays to torch tensors\ntext_tensor = torch.from_numpy(text)\ntext_tensor=text_tensor.unsqueeze(2)\n#text_tensor=text_tensor.permute(0, 1)\nimage_tensor = torch.from_numpy(image)\nimage_tensor=image_tensor.unsqueeze(2)\n#image_tensor=image_tensor.permute(0, 1)\n\n# Define a cross-attention model\nclass CrossAttention(nn.Module):\n    def __init__(self, text_dim, image_dim):\n        super(CrossAttention, self).__init__()\n        self.text_dim = text_dim\n        self.image_dim = image_dim\n        #self.linear_text = nn.Linear(text_dim, image_dim)\n        #self.linear_image = nn.Linear(image_dim, text_dim)\n\n    def forward(self, text_tensor, image_tensor):\n        # Compute attention weights\n        \n        self.text_tensor = text_tensor\n        self.image_tensor = image_tensor\n\n        # Compute attended features\n    \n        attention_score = torch.bmm(image_tensor.transpose(1, 2), text_tensor)\n        \n        attention_score = torch.softmax(attention_score, dim=1)\n        #attention_score = attention_score.unsqueeze(2) # transpose back to [batch_size, sequence_length, feature_dim]\n        context = (torch.bmm(image_tensor,attention_score.transpose(1,2)))\n        #image_attended = torch.matmul(text_att.transpose(0, 1), image)\n        #image_attended = image_attended.transpose(0, 1)\n        print(context.shape)\n        print(image_tensor.shape)\n        # Concatenate attended features\n        attended_features = torch.cat([context, image_tensor], dim=1)\n\n        return attended_features\n\n# Instantiate the cross-attention model\ncross_att = CrossAttention(text_dim=text.shape[1], image_dim=image.shape[1])\n#cross_att = CrossAttention(text_dim=(text.shape), image_dim=(image.shape))\n\n# Perform cross-attention\nattended_features2 = cross_att(text_tensor.float(), image_tensor.float())\n#attended_features2 = attended_features2.squeeze(2)\nprint(attended_features2.shape)\n# Convert the attended features back to numpy array\n#object_class_features = attended_features.detach().numpy()\n#print(object_class_features.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jkB731avNiJ0","outputId":"dd465b56-2900-4532-812f-ae782c13658d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#reconstruct the image after attention\n# Load the pre-trained VAE model\n#vae_model = tf.keras.models.load_model('vae_model.h5')\n# Reconstruct the image from the joint representation\n#reconstructed_img = vae_model.predict(cross_attention)\n#import matplotlib.pyplot as plt\n\n# Rescale the pixel values from -1 to 1 to 0 to 255\n#reconstructed_img = (reconstructed_img[0] + 1) / 2 * 255\n\n# Convert the reconstructed image to integers\n#reconstructed_img = reconstructed_img.astype('uint8')\n\n# Display the reconstructed image\n#plt.imshow(reconstructed_img)\n#plt.show()","metadata":{"id":"wUvzOMStADeG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concatenated_text_global_image=np.concatenate([attended_features3,additive_attention2_features],axis=1)\nprint(concatenated_text_global_image.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Vpl72z2q6X6","outputId":"50ea1e44-4eba-4cb7-d4ef-065726802428"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_shape = (concatenated_text_global_image.shape[1])\nprint(input_shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pvbZGVVt80tZ","outputId":"187d650b-5e42-4cc5-d426-5b01818b1eff"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#concatenated_text_global_image=np.concatenate([attended_features3,additive_attention2_features],axis=1)\ndef bilstmodel(additive_features_text_class):\n    input_shape = (np.expand_dims(additive_features_text_class,axis=2))\n    #reshaped_tensor = input_shape.permute(1, 0, 2)\n    #input_shape=(additive_attention2.shape,)\n    #embedding_sequences = embedding_layer(tx_df)\n    #embedding_sequences = embedding_layer(sequence_input)\n    #txt = SpatialDropout1D(0.1)(input_shape)\n    #txt=Conv1D(100, 4,activation='relu')(txt)#(tembedding_sequences)#(x)\n    txt = Bidirectional(LSTM(32, dropout=0.2, recurrent_dropout=0.2))(input_shape)#(tembedding_sequences)\n    #ttxtb = Dense(256, activation='tanh')(txtb)\n    txt = Dense(1024, activation='relu')(txt)\n    return txt\nbilstm_additive_attention2_features=bilstmodel(numphy_object_image_text_pair)\nprint((np.array(bilstm_additive_attention2_features)).shape)\nbilstm_additive_attention2_features=np.expand_dims(bilstm_additive_attention2_features,axis=2)\nprint(bilstm_additive_attention2_features.shape)\n  ","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kg67U8ALn63z","outputId":"f7bccdcd-de50-4070-a811-66b7e0dae20a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#2nd level cross attention of global_text_features1 and additive_features_text_class1\nimage=global_text_features1\n#text=additive_attention2\n#text=torch.tensor(bilstm_additive_attention2_features)\n#text=attended_features2\ntext=additive_features_text_class1\n#text=additive_attention1\ncross_att2 = CrossAttention(text_dim=text.shape[1], image_dim=image.shape[1])\n#cross_att = CrossAttention(text_dim=(text.shape), image_dim=(image.shape))\n\n# Perform cross-attention\nimage_tensor=image\ntext_tensor=text\nglobal_image_text_pair = cross_att2(text_tensor.float(), image_tensor.float())\n#attended_features = attended_features.squeeze(2)\nprint(global_image_text_pair.shape)\nreshape_global_image_text_pair=crossattention_level2[:, :, 0]\n# Convert the attended features back to numpy array\nnumphy_global_image_text_pair = reshape_global_image_text_pair.detach().numpy()\nprint(numphy_global_image_text_pair.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(attended_features1.shape,additive_features_text_class1.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  2nd cross-attention between cros_attention(attention 1) and additive attention(attention 2) model\nimage=attended_features1\n#text=additive_attention2\n#text=torch.tensor(bilstm_additive_attention2_features)\n#text=attended_features2\ntext=additive_features_text_class1\n#text=additive_attention1\ncross_att2 = CrossAttention(text_dim=text.shape[1], image_dim=image.shape[1])\n#cross_att = CrossAttention(text_dim=(text.shape), image_dim=(image.shape))\n\n# Perform cross-attention\nimage_tensor=image\ntext_tensor=text\nobject_image_text_pair = cross_att2(text_tensor.float(), image_tensor.float())\n#attended_features = attended_features.squeeze(2)\nprint(object_image_text_pair.shape)\nreshape_object_image_text_pair=crossattention_level2[:, :, 0]\n# Convert the attended features back to numpy array\nnumphy_object_image_text_pair = reshape_object_image_text_pair.detach().numpy()\nprint(cross_attention_features_level2.shape)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EQPnNVNyGwZ-","outputId":"51ab1835-61d8-4af7-db10-19d34720858d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  cross-attention between cros_attention(attention 1) and cross attention(global image+text) model\nimage=global_image_text_pair\n#text=additive_attention2\ntext=torch.tensor(bilstm_additive_attention2_features)\n#text=attended_features2\n#text=additive_features1\n#text=additive_attention1\ncross_att2 = CrossAttention(text_dim=text.shape[1], image_dim=image.shape[1])\n#cross_att = CrossAttention(text_dim=(text.shape), image_dim=(image.shape))\n\n# Perform cross-attention\nimage_tensor=image\ntext_tensor=text\ncrossattention_image_text_pair = cross_att2(text_tensor.float(), image_tensor.float())\n#attended_features = attended_features.squeeze(2)\nprint(crossattention_image_text_pair.shape)\ncrossattention_image_text_pairs=crossattention_image_text_pair[:, :, 0]\n# Convert the attended features back to numpy array\ncrossattention_image_text_pairs = crossattention_image_text_pairs.detach().numpy()\nprint(crossattention_image_text_pairs.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DP6V_FFcyUmo","outputId":"d89f35e7-dd9b-48d7-8715-d287cc2cf6d5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n#Train multimodal features creation\n#Cross attention of object_features and class_features with batch size\nimport torch\nimport torch.nn as nn\nimport numpy as np\n#train_classat_features, rtrain_object_features, rtrain_object_features)\n#text=np.array(class_features)#class features\ntext=np.array(train_class_attributes_features) #class-attributes pair features\nprint(text.shape)\nimage=np.array(train_objects_features)\n#image=image_features_arr\n#image=global_features\nprint(image.shape)\n# Convert numpy arrays to torch tensors\ntext_tensor = torch.from_numpy(text)\ntext_tensor=text_tensor.unsqueeze(2)\n#text_tensor=text_tensor.permute(0, 1)\nimage_tensor = torch.from_numpy(image)\nimage_tensor=image_tensor.unsqueeze(2)\n#image_tensor=image_tensor.permute(0, 1)\n\n# Define a cross-attention model\nclass CrossAttention(nn.Module):\n    def __init__(self, text_dim, image_dim):\n        super(CrossAttention, self).__init__()\n        self.text_dim = text_dim\n        self.image_dim = image_dim\n        #self.linear_text = nn.Linear(text_dim, image_dim)\n        #self.linear_image = nn.Linear(image_dim, text_dim)\n\n    def forward(self, text_tensor, image_tensor):\n        # Compute attention weights\n        \n        self.text_tensor = text_tensor\n        self.image_tensor = image_tensor\n\n        # Compute attended features\n    \n        attention_score = torch.bmm(image_tensor.transpose(1, 2), text_tensor)\n        \n        attention_score = torch.softmax(attention_score, dim=1)\n        #attention_score = attention_score.unsqueeze(2) # transpose back to [batch_size, sequence_length, feature_dim]\n        context = (torch.bmm(image_tensor,attention_score.transpose(1,2)))\n        #image_attended = torch.matmul(text_att.transpose(0, 1), image)\n        #image_attended = image_attended.transpose(0, 1)\n        print(context.shape)\n        print(image_tensor.shape)\n        # Concatenate attended features\n        attended_features = torch.cat([context, image_tensor], dim=1)\n\n        return attended_features\n\n# Instantiate the cross-attention model\ncross_att = CrossAttention(text_dim=text.shape[1], image_dim=image.shape[1])\n#cross_att = CrossAttention(text_dim=(text.shape), image_dim=(image.shape))\n\n# Perform cross-attention\nattended_features1 = cross_att(text_tensor.float(), image_tensor.float())\nattended_features = attended_features1.squeeze(2)\nprint(attended_features.shape)\n# Convert the attended features back to numpy array\nobject_class_features = attended_features.detach().numpy()\nprint(object_class_features.shape)\n\n#Additive attention of caption text_features and class_features with batch size\nimport torch\nimport torch.nn as nn\nimport numpy as np\n#train_classat_features, rtrain_object_features, rtrain_object_features)\n#class_text=np.array(class_features)\nclass_text=np.array(train_class_attributes_features)\n#print(text.shape)\ncaption_text=np.array(train_text_features)\n#print(image.shape)\n# Convert numpy arrays to torch tensors\nclass_tensor = torch.from_numpy(text)\nclass_tensor=class_tensor.unsqueeze(2)\n#text_tensor=text_tensor.permute(0, 1)\ncaption_tensor = torch.from_numpy(caption_text)\ncaption_tensor=caption_tensor.unsqueeze(2)\n#image_tensor=image_tensor.permute(0, 1)\n\n# Define a cross-attention model\nclass AdditiveAttention(nn.Module):\n    def __init__(self, class_dim, caption_dim):\n        super(AdditiveAttention, self).__init__()\n        self.class_dim = class_dim\n        self.caption_dim = caption_dim\n        #self.linear_text = nn.Linear(text_dim, image_dim)\n        #self.linear_image = nn.Linear(image_dim, text_dim)\n\n    def forward(self, class_tensor, caption_tensor):\n        # Compute attention weights\n        \n        self.class_tensor = class_tensor\n        self.caption_tensor = caption_tensor\n\n        # Compute attended features\n        attention_score = class_tensor+caption_tensor\n        tanh_score = torch.tanh(attention_score)\n        print(tanh_score.shape)\n        attention_score = torch.softmax(tanh_score, dim=1)\n        print(attention_score.shape)\n        #attention_score = attention_score.unsqueeze(2) # transpose back to [batch_size, sequence_length, feature_dim]\n        context = (torch.bmm(attention_score,class_tensor.transpose(1,2)))\n        #image_attended = torch.matmul(text_att.transpose(0, 1), image)\n        #image_attended = image_attended.transpose(0, 1)\n        print(context.shape)\n        print(caption_tensor.shape)\n        #print(caption_tensor.transpose(1,2).shape)\n        # Concatenate attended features\n        #attended_features = torch.cat([context, caption_tensor.transpose(1,2)], dim=1)\n        attended_features = torch.cat([context[:, :, 0], caption_tensor[:, :, 0]], dim=1)\n\n        #attended_features = torch.cat([context, caption_tensor], dim=0)\n\n        return attended_features\n\n# Instantiate the cross-attention model\nadditive_att = AdditiveAttention(class_dim=class_text.shape[1], caption_dim=caption_text.shape[1])\n#cross_att = CAttention(text_dim=(text.shape), image_dim=(image.shape))\n\n# Perform cross-attention\nadditive_features = additive_att(class_tensor.float(), caption_tensor.float())\nadditive_attention2 = additive_features.unsqueeze(2)\n#print(additive_attention2.shape)\n# Convert the attended features back to numpy array\ntext_class_features = additive_features.detach().numpy()\nprint(text_class_features.shape)\n\n# Instantiate the 2nd cross-attention between cros_attention(attention 1) and additive attention(attention 2) model\nimage=attended_features1\ntext=additive_attention2\ncross_att2 = CrossAttention(text_dim=text.shape[1], image_dim=image.shape[1])\n#cross_att = CrossAttention(text_dim=(text.shape), image_dim=(image.shape))\n\n# Perform cross-attention\nimage_tensor=image\ntext_tensor=text\ncrossattention_level2 = cross_att2(text_tensor.float(), image_tensor.float())\n#attended_features = attended_features.squeeze(2)\nprint(crossattention_level2.shape)\ncross_attention_features_level2=crossattention_level2[:, :, 0]\n# Convert the attended features back to numpy array\ntrain_cross_attention_features_level2 = cross_attention_features_level2.detach().numpy()\nprint(train_cross_attention_features_level2.shape)\n\"\"\"\n","metadata":{"id":"WcHy69QI9bw9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n#Test multimodal features creation\n#Cross attention of object_features and class_features with batch size\nimport torch\nimport torch.nn as nn\nimport numpy as np\n#train_classat_features, rtrain_object_features, rtrain_object_features)\n#text=np.array(class_features)#class features\ntext=np.array(test_class_attributes_features) #class-attributes pair features\nprint(text.shape)\nimage=np.array(test_objects_features)\n#image=image_features_arr\n#image=global_features\nprint(image.shape)\n# Convert numpy arrays to torch tensors\ntext_tensor = torch.from_numpy(text)\ntext_tensor=text_tensor.unsqueeze(2)\n#text_tensor=text_tensor.permute(0, 1)\nimage_tensor = torch.from_numpy(image)\nimage_tensor=image_tensor.unsqueeze(2)\n#image_tensor=image_tensor.permute(0, 1)\n\n# Define a cross-attention model\nclass CrossAttention(nn.Module):\n    def __init__(self, text_dim, image_dim):\n        super(CrossAttention, self).__init__()\n        self.text_dim = text_dim\n        self.image_dim = image_dim\n        #self.linear_text = nn.Linear(text_dim, image_dim)\n        #self.linear_image = nn.Linear(image_dim, text_dim)\n\n    def forward(self, text_tensor, image_tensor):\n        # Compute attention weights\n        \n        self.text_tensor = text_tensor\n        self.image_tensor = image_tensor\n\n        # Compute attended features\n    \n        attention_score = torch.bmm(image_tensor.transpose(1, 2), text_tensor)\n        \n        attention_score = torch.softmax(attention_score, dim=1)\n        #attention_score = attention_score.unsqueeze(2) # transpose back to [batch_size, sequence_length, feature_dim]\n        context = (torch.bmm(image_tensor,attention_score.transpose(1,2)))\n        #image_attended = torch.matmul(text_att.transpose(0, 1), image)\n        #image_attended = image_attended.transpose(0, 1)\n        print(context.shape)\n        print(image_tensor.shape)\n        # Concatenate attended features\n        attended_features = torch.cat([context, image_tensor], dim=1)\n\n        return attended_features\n\n# Instantiate the cross-attention model\ncross_att = CrossAttention(text_dim=text.shape[1], image_dim=image.shape[1])\n#cross_att = CrossAttention(text_dim=(text.shape), image_dim=(image.shape))\n\n# Perform cross-attention\nattended_features1 = cross_att(text_tensor.float(), image_tensor.float())\nattended_features = attended_features1.squeeze(2)\nprint(attended_features.shape)\n# Convert the attended features back to numpy array\nobject_class_features = attended_features.detach().numpy()\nprint(object_class_features.shape)\n\n#Additive attention of caption text_features and class_features with batch size\nimport torch\nimport torch.nn as nn\nimport numpy as np\n#train_classat_features, rtrain_object_features, rtrain_object_features)\n#class_text=np.array(class_features)\nclass_text=np.array(test_class_attributes_features)\n#print(text.shape)\ncaption_text=np.array(test_text_features)\n#print(image.shape)\n# Convert numpy arrays to torch tensors\nclass_tensor = torch.from_numpy(text)\nclass_tensor=class_tensor.unsqueeze(2)\n#text_tensor=text_tensor.permute(0, 1)\ncaption_tensor = torch.from_numpy(caption_text)\ncaption_tensor=caption_tensor.unsqueeze(2)\n#image_tensor=image_tensor.permute(0, 1)\n\n# Define a cross-attention model\nclass AdditiveAttention(nn.Module):\n    def __init__(self, class_dim, caption_dim):\n        super(AdditiveAttention, self).__init__()\n        self.class_dim = class_dim\n        self.caption_dim = caption_dim\n        #self.linear_text = nn.Linear(text_dim, image_dim)\n        #self.linear_image = nn.Linear(image_dim, text_dim)\n\n    def forward(self, class_tensor, caption_tensor):\n        # Compute attention weights\n        \n        self.class_tensor = class_tensor\n        self.caption_tensor = caption_tensor\n\n        # Compute attended features\n        attention_score = class_tensor+caption_tensor\n        tanh_score = torch.tanh(attention_score)\n        print(tanh_score.shape)\n        attention_score = torch.softmax(tanh_score, dim=1)\n        print(attention_score.shape)\n        #attention_score = attention_score.unsqueeze(2) # transpose back to [batch_size, sequence_length, feature_dim]\n        context = (torch.bmm(attention_score,class_tensor.transpose(1,2)))\n        #image_attended = torch.matmul(text_att.transpose(0, 1), image)\n        #image_attended = image_attended.transpose(0, 1)\n        print(context.shape)\n        print(caption_tensor.shape)\n        #print(caption_tensor.transpose(1,2).shape)\n        # Concatenate attended features\n        #attended_features = torch.cat([context, caption_tensor.transpose(1,2)], dim=1)\n        attended_features = torch.cat([context[:, :, 0], caption_tensor[:, :, 0]], dim=1)\n\n        #attended_features = torch.cat([context, caption_tensor], dim=0)\n\n        return attended_features\n\n# Instantiate the cross-attention model\nadditive_att = AdditiveAttention(class_dim=class_text.shape[1], caption_dim=caption_text.shape[1])\n#cross_att = CAttention(text_dim=(text.shape), image_dim=(image.shape))\n\n# Perform cross-attention\nadditive_features = additive_att(class_tensor.float(), caption_tensor.float())\nadditive_attention2 = additive_features.unsqueeze(2)\n#print(additive_attention2.shape)\n# Convert the attended features back to numpy array\ntext_class_features = additive_features.detach().numpy()\nprint(text_class_features.shape)\n\n# Instantiate the 2nd cross-attention between cros_attention(attention 1) and additive attention(attention 2) model\nimage=attended_features1\ntext=additive_attention2\ncross_att2 = CrossAttention(text_dim=text.shape[1], image_dim=image.shape[1])\n#cross_att = CrossAttention(text_dim=(text.shape), image_dim=(image.shape))\n\n# Perform cross-attention\nimage_tensor=image\ntext_tensor=text\ncrossattention_level2 = cross_att2(text_tensor.float(), image_tensor.float())\n#attended_features = attended_features.squeeze(2)\nprint(crossattention_level2.shape)\ncross_attention_features_level2=crossattention_level2[:, :, 0]\n# Convert the attended features back to numpy array\ntest_cross_attention_features_level2 = cross_attention_features_level2.detach().numpy()\nprint(test_cross_attention_features_level2.shape)\n\"\"\"\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-l0PCi_P_9hV","outputId":"a0cd73e6-07b1-4ec3-bf40-c635508f6704"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(text_features.shape)","metadata":{"id":"wlilA0BW-RS9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a19aa004-8059-434d-9c6f-0a2747c809dd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Simple Concatenation of Image and  **","metadata":{"id":"GtaCvrJf2ZwX"}},{"cell_type":"code","source":"#all experiment with balance sampling\n#combined global(image features), capton text features\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\n\n\n#all experiment with balance sampling\n#combined global(image features), capton text features\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\nfrom sklearn import svm\n#with strategy.scope(): #when TPU is used\n#with tf.device('/device:GPU:0'): #when GPU is used\nros = RandomOverSampler()\n#combined_features=np.concatenate([object_class_features,text_features,global_features], axis=1)\n#combined_features = np.concatenate([cross_attention_features_level2, cbt_class_attributes_features,attended_features3], axis=1)\n#combined_features = np.concatenate([cross_attention_features_level2, cbt_class_attributes_features,attended_features3,global_features,text_features], axis=1)\n#combined_features = np.concatenate([crossattention_image_text_pairs, global_features,cbt_class_attributes_features], axis=1)\ncombined_features = np.concatenate([text_features,class_attributes_features,global_features,local_object_features],axis=1)\n#combined_features_ros, target_labels_ros = ros.fit_resample(combined_features, target_labels)\n#X_train, X_test, y_train, y_test = train_test_split(combined_features_ros, target_labels_ros, test_size=0.1)\nX_train, X_test, y_train, y_test = train_test_split(combined_features, target_labels, test_size=0.1)\n#X_train, X_test, y_train, y_test = train_test_split(combined_features, df.image_label, test_size=0.1)\n#X_train, y_train = ros.fit_resample(X_train, y_train) \n#X_test, y_test = ros.fit_resample(X_train, y_train) \nprint(X_train.shape,X_test.shape,y_train.shape,y_test.shape) ","metadata":{"id":"xUZJniZ3SHPo","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e89c9731-e44f-4f63-cb2f-454df4ee3123","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import svm\nwith tf.device('/device:GPU:0'):\n# create SVM classifier\n  clf = svm.SVC(kernel='linear', C=1, probability=True)\n\n  # train the classifier on the training data\n  clf.fit(X_train, y_train)\n\n  # evaluate the classifier on the test data\n  #print(accuracy)\n  # evaluate the classifier on the test data\n  accuracy = clf.score(X_test, y_test)\n  predicted = clf.predict(X_test)\n\n  #ypredicted = np.argmax(predicted,axis=1)\n  #y_predicted = np.where(predicted>0.5,1,0)\n  print('Results of SVM')\n  from sklearn.metrics import classification_report\n  print(classification_report(y_test,predicted,digits=4))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a4h6LqkbMyHW","outputId":"a3311cc3-cff5-4dfb-e3a1-e79ed05bd5d4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## get dummies values for labels\ny_train=pd.get_dummies(y_train)\ndef get_label_code(text):\n    if text=='positive':\n        return 2\n    elif text=='neutral':\n        return 1\n    else :\n        return 0\nmy_vectorized_function = np.vectorize(get_label_code)\ny_test = my_vectorized_function(y_test)\n#y_test=y_test.apply(get_label_code)\nX_train = np.expand_dims(X_train, axis=-1)","metadata":{"id":"gowAx-1KXrHC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"h_wjBvdYCf6C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"Ihm259--iUgA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\n# assume X_train, y_train, X_test, y_test are NumPy arrays containing the training and test data\ninput_shape = (combined_features.shape[1], 1) # assuming the data is a 1D sequence of length 1056\n\n# define the CNN1D model\ncnn_model = Sequential()\ncnn_model.add(Conv1D(filters=32, kernel_size=4, activation='relu', input_shape=input_shape))\ncnn_model.add(MaxPooling1D(pool_size=2))\ncnn_model.add(Conv1D(filters=64, kernel_size=4, activation='relu'))\n#cnn_model.add(MaxPooling1D(pool_size=2))\ncnn_model.add(Flatten())\n#cnn_model.add(Dense(128, activation='relu'))\ncnn_model.add(Dropout(0.5))\ncnn_model.add(Dense(3, activation='softmax'))\n#cnn_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['categorical_accuracy'])\ncnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy',\n             metrics=['categorical_accuracy'])\n#####\n#X_train = np.expand_dims(X_train, axis=-1)\ncnn_history=cnn_model.fit(x=X_train,validation_split=0.1,y= y_train,batch_size=128, epochs=10)\n#com_history=fusion_modelfit(x=X_train, y_train, epochs=10,batch_size=128, validation_split=0.1,validation_data=(X_test, y_test))\n\n#accuracy vs loss function\ns, (at, al) = plt.subplots(2,1)\nat.plot(cnn_history.history['categorical_accuracy'], c= 'b')\nat.plot(cnn_history.history['val_categorical_accuracy'], c='r')\nat.set_title('model accuracy')\nat.set_ylabel('accuracy')\nat.set_xlabel('epoch')\nat.legend(['Fusion_Res_cnn_train', 'Fusion_Res_cnn_val'], loc='upper left')\n\nal.plot(cnn_history.history['loss'], c='m')\nal.plot(cnn_history.history['val_loss'], c='c')\nal.set_title('model loss')\nal.set_ylabel('loss')\nal.set_xlabel('epoch')\nal.legend(['train', 'val'], loc = 'upper left')\ncpredicted = cnn_model.predict(X_test)\n#com_score=fusion_model.evaluate(y_test)\n#print('Test Loss:', com_score[0])\n#print('Test accuracy:', com_score[1])\ncypredicted = np.argmax(cpredicted,axis=1)\n#y_predicted = np.where(predicted>0.5,1,0)\nprint('Results of CNN model')\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,cypredicted,digits=4))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ZcKSmZUwL6iT","outputId":"26897f88-9354-438e-d7f7-7318d7de784e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\n# assume X_train, y_train, X_test, y_test are NumPy arrays containing the training and test data\ninput_shape = (combined_features.shape[1], 1) # assuming the data is a 1D sequence of length 1056\n\n# define the CNN1D model\ncnn_model = Sequential()\ncnn_model.add(Conv1D(filters=32, kernel_size=4, activation='relu', input_shape=input_shape))\ncnn_model.add(MaxPooling1D(pool_size=2))\n#cnn_model.add(Conv1D(filters=64, kernel_size=4, activation='relu'))\n#cnn_model.add(MaxPooling1D(pool_size=2))\ncnn_model.add(Flatten())\n#cnn_model.add(Dense(128, activation='relu'))\n#cnn_model.add(Dropout(0.5))\ncnn_model.add(Dense(3, activation='softmax'))\n#cnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\ncnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy',\n            metrics=['categorical_accuracy'])\n#####\n#X_train = np.expand_dims(X_train, axis=-1)\ncnn_history=cnn_model.fit(x=X_train,validation_split=0.1,y= y_train,batch_size=128, epochs=20)\n#com_history=fusion_modelfit(x=X_train, y_train, epochs=10,batch_size=128, validation_split=0.1,validation_data=(X_test, y_test))\n\n#accuracy vs loss function\ns, (at, al) = plt.subplots(2,1)\nat.plot(cnn_history.history['categorical_accuracy'], c= 'b')\nat.plot(cnn_history.history['val_categorical_accuracy'], c='r')\nat.set_title('model accuracy')\nat.set_ylabel('accuracy')\nat.set_xlabel('epoch')\nat.legend(['Fusion_Res_cnn_train', 'Fusion_Res_cnn_val'], loc='upper left')\n\nal.plot(cnn_history.history['loss'], c='m')\nal.plot(cnn_history.history['val_loss'], c='c')\nal.set_title('model loss')\nal.set_ylabel('loss')\nal.set_xlabel('epoch')\nal.legend(['train', 'val'], loc = 'upper left')\ncpredicted = cnn_model.predict(X_test)\n#com_score=fusion_model.evaluate(y_test)\n#print('Test Loss:', com_score[0])\n#print('Test accuracy:', com_score[1])\ncypredicted = np.argmax(cpredicted,axis=1)\n#y_predicted = np.where(predicted>0.5,1,0)\nprint('Results of CNN model')\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,cypredicted,digits=4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define the BiLSTM model\nfrom keras.models import Sequential\n# assume X_train, y_train, X_test, y_test are NumPy arrays containing the training and test data\ninput_shape = (combined_features.shape[1], 1)\nlstm_model = Sequential()\nlstm_model.add(Bidirectional(LSTM(64,dropout=0.5,recurrent_dropout=0.5), input_shape=input_shape))\n#lstm_model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=input_shape))\n#lstm_model.add(Bidirectional(LSTM(64)))\nlstm_model.add(Dense(3, activation='softmax'))\n#lstm_model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['categorical_accuracy'])\nlstm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss='categorical_crossentropy',\n            metrics=['categorical_accuracy'])\n  \nwith tf.device('/device:GPU:0'): #when GPU is used\n  #Excecute Bilstm\n  lstm_history=lstm_model.fit(x=X_train, y=y_train,validation_split=0.1,batch_size=128,  epochs=5)\n  #com_history=fusion_modelfit(x=X_train, y_train, epochs=10,batch_size=128, validation_split=0.1,validation_data=(X_test, y_test))\n\n  #accuracy vs loss function\n  s, (at, al) = plt.subplots(2,1)\n  at.plot(lstm_history.history['categorical_accuracy'], c= 'b')\n  at.plot(lstm_history.history['val_categorical_accuracy'], c='r')\n  at.set_title('model accuracy')\n  at.set_ylabel('accuracy')\n  at.set_xlabel('epoch')\n  at.legend(['Fusion_bilstm_train', 'Fusion_bilstm_val'], loc='upper left')\n\n  al.plot(lstm_history.history['loss'], c='m')\n  al.plot(lstm_history.history['val_loss'], c='c')\n  al.set_title('model loss')\n  al.set_ylabel('loss')\n  al.set_xlabel('epoch')\n  al.legend(['train', 'val'], loc = 'upper left')\n  cpredicted = lstm_model.predict(X_test)\n  #com_score=fusion_model.evaluate(y_test)\n  #print('Test Loss:', com_score[0])\n  #print('Test accuracy:', com_score[1])\n  cypredicted = np.argmax(cpredicted,axis=1)\n  #y_predicted = np.where(predicted>0.5,1,0)\n  print('Results of Bilstm model')\n  from sklearn.metrics import classification_report\n  print(classification_report(y_test,cypredicted))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n\n# Set the initial learning rate, decay steps, decay rate, and weight decay\ninitial_learning_rate = 0.001\ndecay_steps = 10\ndecay_rate = 0.1\nweight_decay = 1e-3\n\n# Create the learning rate schedule\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate,\n    decay_steps=decay_steps,\n    decay_rate=decay_rate\n)\n\n# Create the optimizer with weight decay\noptimizer = tf.keras.optimizers.legacy.Adam(\n    learning_rate=lr_schedule,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    amsgrad=False,\n    decay=weight_decay\n)\n\n# Compile the model\n#model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Fit the model\n#model.fit(x_train, y_train, epochs=num_epochs, batch_size=batch_size)\n\"\"\"\n","metadata":{"id":"hXuHWG-QiU_G"},"execution_count":null,"outputs":[]}]}