{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8240,"sourceType":"datasetVersion","datasetId":5504},{"sourceId":3501056,"sourceType":"datasetVersion","datasetId":2107125},{"sourceId":4157148,"sourceType":"datasetVersion","datasetId":2454281},{"sourceId":4572036,"sourceType":"datasetVersion","datasetId":2667369},{"sourceId":4647875,"sourceType":"datasetVersion","datasetId":2701120},{"sourceId":6200439,"sourceType":"datasetVersion","datasetId":3559855},{"sourceId":7619387,"sourceType":"datasetVersion","datasetId":4437970}],"dockerImageVersionId":30369,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"                                                                                                                                                                                        # This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.090498,"end_time":"2022-09-18T13:22:55.807293","exception":false,"start_time":"2022-09-18T13:22:55.716795","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T11:46:59.349517Z","iopub.execute_input":"2024-04-08T11:46:59.349895Z","iopub.status.idle":"2024-04-08T11:46:59.355753Z","shell.execute_reply.started":"2024-04-08T11:46:59.349864Z","shell.execute_reply":"2024-04-08T11:46:59.354657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#def get_img_ext_code(id):\n#    newid=str(id)\n#    return (newid+'.jpg')\n#mdff['img_path']=mdff['ID'].apply(get_img_ext_code)\n#dff['vlabel']=dff['image_label'].apply(get_label_code)\n#mdf1=pd.merge(mdff,df,on='img_path',how='inner')\n#edf.rename(columns = {'FileName':'file_name'}, inplace = True)\n#edf\n#mdf1=mdf1[(mdf1.number_of_bbox)==]\n#ndf.shape\n#mdf1[:2]\n","metadata":{"papermill":{"duration":0.082341,"end_time":"2022-09-18T13:22:55.965462","exception":false,"start_time":"2022-09-18T13:22:55.883121","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''''import matplotlib.pyplot as plt\nval_count = ((edf['text_label'].value_counts()/edf.shape[0])*100)\n#vacine.plot(figsize=(16, 4), kind='bar', x='Vaccine_name', y='% of frequency', title='Covid19 Vaccine 2020')\nplt.figure(figsize=(8,4))\nplt.xlabel('<-----Proposed Hybrid Ternary Method----->')\nplt.ylabel('% of Sentiments')\nplt.bar(val_count.index, val_count.values)\n#plt.title(\"% Sentiments of Data Distribution\")'''","metadata":{"papermill":{"duration":0.084908,"end_time":"2022-09-18T13:22:56.127200","exception":false,"start_time":"2022-09-18T13:22:56.042292","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df.to_csv('./simpson_four_object.csv',index=False)\n#df.to_csv('./mvsa_class_att_afinn.csv',index=False)","metadata":{"papermill":{"duration":0.083201,"end_time":"2022-09-18T13:22:56.284571","exception":false,"start_time":"2022-09-18T13:22:56.201370","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****","metadata":{"papermill":{"duration":0.07625,"end_time":"2022-09-18T13:22:56.436264","exception":false,"start_time":"2022-09-18T13:22:56.360014","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"****DATA CLeaning****","metadata":{"papermill":{"duration":0.07569,"end_time":"2022-09-18T13:22:56.587334","exception":false,"start_time":"2022-09-18T13:22:56.511644","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#!pip install text_hammer","metadata":{"papermill":{"duration":0.083248,"end_time":"2022-09-18T13:22:56.746382","exception":false,"start_time":"2022-09-18T13:22:56.663134","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''#preproccesing for cleaning\n#%%time\nimport text_hammer as th\nfrom tqdm._tqdm_notebook import tqdm_notebook\ntqdm_notebook.pandas()\n\ndef text_preprocessing(df,col_name):\n    column = col_name\n    df[column] = df[column].progress_apply(lambda x:str(x).lower())\n    df[column] = df[column].progress_apply(lambda x: th.cont_exp(x)) #you're -> you are; i'm -> i am\n    df[column] = df[column].progress_apply(lambda x: th.remove_emails(x))\n    df[column] = df[column].progress_apply(lambda x: th.remove_html_tags(x))\n    df[column] = df[column].progress_apply(lambda x: th.remove_stopwords(x))\n\n    df[column] = df[column].progress_apply(lambda x: th.remove_special_chars(x))\n    df[column] = df[column].progress_apply(lambda x: th.remove_accented_chars(x))\n    df[column] = df[column].progress_apply(lambda x: th.make_base(x)) #ran -> run,\n    return(df)\n    #return(df[column])'''","metadata":{"papermill":{"duration":0.084207,"end_time":"2022-09-18T13:22:56.906772","exception":false,"start_time":"2022-09-18T13:22:56.822565","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mdf1 = text_preprocessing(mdf1,'object_list')\n#dff.drop(['text_x','text_label','image_label'],axis='columns',inplace=True)\n#mdf1=pd.merge(dff1,dff,on='ID',how='inner')\n#merge two columns\n#train_cleaned_df['short_text40'] = train_cleaned_df['Text'].apply(lambda x: x[:] if (len(x.split())<40) else (\" \".join(x.split()[:40])))\n#mdf1[\"classes_with_ctext\"]=mdf1[\"object_list\"].str.cat(mdf1[\"ctext\"])\n#bdf=df[df['number_of_bbox']==1]\n#import re\n#Clean brakets braces\n#bdf[\"clean_bbox\"] = bdf[\"bbox\"].apply(lambda x: re.sub(\"[\\([{})\\]]\", ' ', str(x)).strip())\n#bdf[\"clean_bbox\"] = bdf[\"clean_bbox\"].apply(lambda x: str(x).strip(), x.split(','))\n#remove none values columns\n#mdf=mdf.dropna()\n#df[\"clean_bbox\"]\n#bdf[:2]\n","metadata":{"papermill":{"duration":0.082549,"end_time":"2022-09-18T13:22:57.070382","exception":false,"start_time":"2022-09-18T13:22:56.987833","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df.joint_sentiment.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Import Files**","metadata":{}},{"cell_type":"code","source":"#df=pd.read_csv('../input/mvsa-attributes/mvsa_attr_info1.csv')\n#df=pd.read_csv('/kaggle/input/mvsa-single-scene/mvsa_attr_info1_scene.csv')\ndf1=pd.read_csv('/kaggle/input/simpson-attrbutes/simpson_attr_info.csv')\ndf2=pd.read_csv('/kaggle/input/simpson-scene-clean/simpson_scene_clean.csv')\n#df=pd.read_csv('../input/mvsa-single-afinn-labelling/mvsa_single_affin_labelling.csv')\ndf=pd.merge(df1,df2,on='img_path',how='inner')\n#df=df.dropna()\ndf.rename(columns = {'visual_label':'image_label'}, inplace = True)\n#df=df.dropna()\ndf[:2]\n#mdf=mdf.dropna()\n#mdf.shape","metadata":{"papermill":{"duration":0.194692,"end_time":"2022-09-18T13:22:57.338913","exception":false,"start_time":"2022-09-18T13:22:57.144221","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T15:10:14.753686Z","iopub.execute_input":"2024-04-08T15:10:14.754107Z","iopub.status.idle":"2024-04-08T15:10:14.946761Z","shell.execute_reply.started":"2024-04-08T15:10:14.754065Z","shell.execute_reply":"2024-04-08T15:10:14.945755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.joint_sentiment.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:39:02.724120Z","iopub.execute_input":"2024-02-26T07:39:02.724552Z","iopub.status.idle":"2024-02-26T07:39:02.736281Z","shell.execute_reply.started":"2024-02-26T07:39:02.724516Z","shell.execute_reply":"2024-02-26T07:39:02.735041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.image_label.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T10:42:07.089015Z","iopub.execute_input":"2024-04-08T10:42:07.089426Z","iopub.status.idle":"2024-04-08T10:42:07.100601Z","shell.execute_reply.started":"2024-04-08T10:42:07.089379Z","shell.execute_reply":"2024-04-08T10:42:07.099445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df=df.dropna()\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2024-02-25T22:38:36.505940Z","iopub.execute_input":"2024-02-25T22:38:36.506353Z","iopub.status.idle":"2024-02-25T22:38:36.512656Z","shell.execute_reply.started":"2024-02-25T22:38:36.506318Z","shell.execute_reply":"2024-02-25T22:38:36.511772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#For Simpson dataset\ndf['ctext'] = df['text'].apply(lambda x: x[:] if (len(x.split())<20) else (\" \".join(x.split()[:20])))","metadata":{"execution":{"iopub.status.busy":"2024-02-26T13:34:32.655894Z","iopub.execute_input":"2024-02-26T13:34:32.656717Z","iopub.status.idle":"2024-02-26T13:34:32.668300Z","shell.execute_reply.started":"2024-02-26T13:34:32.656679Z","shell.execute_reply":"2024-02-26T13:34:32.667234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\ndf[\"class_att1\"] = df[\"class_att\"].apply(lambda x: re.sub(\"[\\[{','}\\]]\", ' ', str(x)).strip())\ndf[\"scene_res50\"] = df[\"scene_res50\"].apply(lambda x: re.sub(\"[\\[{','}\\]]\", ' ', str(x)).strip())\ndf[\"class_att_res50\"] = df[\"class_att\"].str.cat(df[\"scene_res50\"])\ndf[\"class_att_res50\"] = df[\"class_att_res50\"].apply(lambda x: re.sub(\"[\\[{','}\\]]\", ' ', str(x)).strip())\n#df[\"cbts\"] = df[\"ctext\"].str.cat(df[\"class_att\"])\n#df[\"cbts1\"] = df[\"ctext\"].str.cat(df[\"class_att_res50\"])\n#df[\"cbts1\"] = df[\"class_att_res50\"].str.cat(df[\"ctext\"])\n#df[\"class_att1\"] = df[\"class_att\"].apply(lambda x: re.sub(\"[\\[{','}\\]]\", ' ', str(x)).strip())\ndf[:2]","metadata":{"execution":{"iopub.status.busy":"2024-04-08T15:10:43.621688Z","iopub.execute_input":"2024-04-08T15:10:43.622097Z","iopub.status.idle":"2024-04-08T15:10:43.818704Z","shell.execute_reply.started":"2024-04-08T15:10:43.622059Z","shell.execute_reply":"2024-04-08T15:10:43.817732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#remove duplicate pairs of words from class_att\ndef drop_dupli(text):\n    #seen = set()\n    result = []\n    seen=[]\n    #c=0\n    for item in text.split(): \n       # seen=[]\n        if item not in seen:\n            #seen.add(item)\n            seen.append(item)\n            result.append(item)\n    return \" \".join(result)\ndf['class_att1'] = df['class_att'].apply(drop_dupli)\n#df['class_att1'] = df['class_att'].apply(lambda x: list(dict.fromkeys(x)))\ndf.class_att1\n#df['newame'] = df['Names'].apply(lambda x: list(dict.fromkeys(x)))\n#print (df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#For Simpson dataset\ndf['text'] = df['text'].apply(lambda x: x[:] if (len(x.split())<20) else (\" \".join(x.split()[:20])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df.joint_sentiment.value_counts()\n#df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df.to_csv('./mvsa_class_attributeswith affin.csv,index=false')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''''df=mdf[mdf['image_label']==mdf['text_label']]\ndf=df.dropna()\ndf.shape\n#df[:2]'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df1 = text_preprocessing(df,'class_att')\n#df1 = text_preprocessing(df,'object_names')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df1[\"object_attr_ctext\"]=df1[\"object_names\"].str.cat(df1[\"att_name\"])\ndf[\"object_attr_text\"]=df[\"class_att\"].str.cat(df[\"text\"])\ndf[\"object_attr1_text\"]=df[\"class_att1\"].str.cat(df[\"text\"])\n#df1[:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n#df[\"object_names\"] = df[\"object_names\"].apply(lambda x: re.sub(\"[\\[{','}\\]]\", ' ', str(x)).strip())\n#df[\"att_name\"] = df[\"att_name\"].apply(lambda x: re.sub(\"[\\[{','}\\]]\", ' ', str(x)).strip())\ndf[\"text\"] = df[\"text\"].apply(lambda x: re.sub(\"[\\[{}\\]]\", ' ', str(x)).strip())","metadata":{"execution":{"iopub.status.busy":"2023-01-24T19:15:08.100149Z","iopub.execute_input":"2023-01-24T19:15:08.100523Z","iopub.status.idle":"2023-01-24T19:15:08.120265Z","shell.execute_reply.started":"2023-01-24T19:15:08.100489Z","shell.execute_reply":"2023-01-24T19:15:08.119365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\ndf[\"object_names\"] = df[\"object_names\"].apply(lambda x: re.sub(\"[\\[{','}\\]]\", ' ', str(x)).strip())\ndf[\"att_name\"] = df[\"att_name\"].apply(lambda x: re.sub(\"[\\[{','}\\]]\", ' ', str(x)).strip())\ndf[\"text\"] = df[\"text\"].apply(lambda x: re.sub(\"[\\[{}\\]]\", ' ', str(x)).strip())\ndf[\"class_att\"] = df[\"class_att\"].apply(lambda x: re.sub(\"[\\[{','}\\]]\", ' ', str(x)).strip())\ndf[\"object_attr1_text\"] = df[\"object_attr1_text\"].apply(lambda x: re.sub(\"[\\[{','}\\]]\", ' ', str(x)).strip())\ndf[\"object_attr_text\"] = df[\"object_attr_text\"].apply(lambda x: re.sub(\"[\\[{','}\\]]\", ' ', str(x)).strip())\ndf[:2]\n#df3=dff[dff['number_of_bbox']==3]\n#df3[\"clean_bbox\"] = df3[\"bbox\"].apply(lambda x: re.sub(\"[\\[{}\\]]\", ' ', str(x)).strip())\n#df3[:2]\n#df3.to_csv('./mvsa_3object.csv',index=False)","metadata":{"papermill":{"duration":0.129364,"end_time":"2022-09-18T13:22:58.428841","exception":false,"start_time":"2022-09-18T13:22:58.299477","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Blance data for MVSA-Multiple\n# Downsample the data by randomly selecting 10% of the rows\n#downsampled_data = data.sample(frac=0.1)\n\n# Upsample the data by duplicating each row two times\nneg=df[df['joint_sentiment']=='negative']\n#upsampled_neg = neg.reindex(neg.index.repeat(6))\n#upsampled_neg=upsampled_neg.iloc[:5000,:]\nprint(neg.shape)\nnut=df[df['joint_sentiment']=='neutral']\ndownsampled_nut=nut.iloc[:798,:]\n#upsampled_nut = nut.reindex(nut.index.repeat(2))\n#print(upsampled_nut.shape)\n#downsampled_nut = nut.sample(frac=0.1)\n#downsampled_nut=pd.concat([downsampled_nut,nut])\n#print(downsampled_nut.shape)\npos=df[df['joint_sentiment']=='positive']\ndownsampled_pos=pos.iloc[:798,:]\n#upsampled_pos = pos.reindex(pos.index.repeat(3))\n#downsampled_pos = pos.sample(frac=0.8)\n#concat three sampling\n#df=pd.concat([downsampled_pos,upsampled_nut,upsampled_neg])\ndff=pd.concat([downsampled_pos,downsampled_nut,neg])\nprint(df.shape)\ndff.joint_sentiment.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-02-26T12:58:31.854125Z","iopub.execute_input":"2024-02-26T12:58:31.855217Z","iopub.status.idle":"2024-02-26T12:58:31.879456Z","shell.execute_reply.started":"2024-02-26T12:58:31.855173Z","shell.execute_reply":"2024-02-26T12:58:31.878189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split each dataframe into training and testing sets\nfrom sklearn.model_selection import train_test_split\npos_train, pos_test = train_test_split(downsampled_pos, test_size=0.2)\nneg_train, neg_test = train_test_split(neg, test_size=0.2)\nnut_train, nut_test = train_test_split(downsampled_nut, test_size=0.2)\n\n# Combine the training sets into one dataframe\ntrain_df = pd.concat([pos_train, neg_train, nut_train], ignore_index=True)\n\n# Combine the testing sets into one dataframe\ntest_df = pd.concat([pos_test, neg_test, nut_test], ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T12:58:56.891852Z","iopub.execute_input":"2024-02-26T12:58:56.892270Z","iopub.status.idle":"2024-02-26T12:58:56.909597Z","shell.execute_reply.started":"2024-02-26T12:58:56.892229Z","shell.execute_reply":"2024-02-26T12:58:56.908708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Afinn labelling**","metadata":{}},{"cell_type":"code","source":"print(train_df.shape,test_df.shape)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T12:59:03.321752Z","iopub.execute_input":"2024-02-26T12:59:03.322661Z","iopub.status.idle":"2024-02-26T12:59:03.327713Z","shell.execute_reply.started":"2024-02-26T12:59:03.322623Z","shell.execute_reply":"2024-02-26T12:59:03.326658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[:2]","metadata":{"execution":{"iopub.status.busy":"2024-02-26T12:45:51.567628Z","iopub.execute_input":"2024-02-26T12:45:51.568005Z","iopub.status.idle":"2024-02-26T12:45:51.590773Z","shell.execute_reply.started":"2024-02-26T12:45:51.567974Z","shell.execute_reply":"2024-02-26T12:45:51.589782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install afinn","metadata":{"execution":{"iopub.status.busy":"2024-04-08T11:47:35.615511Z","iopub.execute_input":"2024-04-08T11:47:35.615909Z","iopub.status.idle":"2024-04-08T11:47:46.964929Z","shell.execute_reply.started":"2024-04-08T11:47:35.615876Z","shell.execute_reply":"2024-04-08T11:47:46.963689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom afinn import Afinn \n# compute afinn scores (polarity) and labels\n#from afinn import Afinn \nafn = Afinn()\n#df['ruafin_score']           = df['ruser'].apply(lambda x: afn.score(x))\ndf['ruafin_score']           = df['class_att_res50'].apply(lambda x: afn.score(x))\n#df['ruafin_score']           = df['clattscene_drop_color'].apply(lambda x: afn.score(x))\n#scores = [afn.score(article) for article in news_df]\ndef afine_label(score):\n    if score > 0 : \n        return \"positive\" \n  \n    elif score < 0 : \n        return \"negative\" \n  \n    else : \n        return \"neutral\" \n#print(sid.polarity_scores('ruined'))\ndf['af']=df['ruafin_score'].apply(afine_label)\ndf.af.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T15:12:17.808725Z","iopub.execute_input":"2024-04-08T15:12:17.809523Z","iopub.status.idle":"2024-04-08T15:12:22.457772Z","shell.execute_reply.started":"2024-04-08T15:12:17.809487Z","shell.execute_reply":"2024-04-08T15:12:22.456781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=df.dropna()\ndf.afc.value_counts()\n#dff.number_of_bbox.value_counts()","metadata":{"papermill":{"duration":0.082321,"end_time":"2022-09-18T13:22:58.586806","exception":false,"start_time":"2022-09-18T13:22:58.504485","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train Test Splitting**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_df, test_df = train_test_split(df, test_size=0.20)\n#train_df, val_df = train_test_split(train_dff, test_size=0.1)\nprint(train_df.shape,test_df.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T16:56:37.774239Z","iopub.execute_input":"2024-04-08T16:56:37.774694Z","iopub.status.idle":"2024-04-08T16:56:37.785566Z","shell.execute_reply.started":"2024-04-08T16:56:37.774657Z","shell.execute_reply":"2024-04-08T16:56:37.784532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''from sklearn.model_selection import train_test_split\ntrain_dff, test_df = train_test_split(df, test_size=0.1,random_state=7)\ntrain_df, val_df = train_test_split(train_dff, test_size=0.1)\nprint(train_df.shape,val_df.shape,test_df.shape)\n#train_df,test_df,vx_train,vx_test,ox_train,ox_test=train_test_split(bdf,imgar,object_ar,test_size=0.2,random_state=7)\n#train_df,test_df=train_test_split(edf,test_size=0.2,random_state=7,stratify=edf['joint_label'])'''","metadata":{"papermill":{"duration":0.816924,"end_time":"2022-09-18T13:23:15.616389","exception":false,"start_time":"2022-09-18T13:23:14.799465","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#y_train=pd.get_dummies(train_df.af)\n#y_val=pd.get_dummies(val_df.joint_sentiment)\n#y_test=pd.get_dummies(test_df.af)\n#y_train=pd.get_dummies(train_df.joint_sentiment)\n#y_val=pd.get_dummies(val_df.joint_sentiment)\n#y_test=pd.get_dummies(test_df.joint_sentiment)\ny_train=pd.get_dummies(train_df.image_label)\ny_test=pd.get_dummies(train_df.image_label)\n#print(y_train.shape,y_val.shape,y_test.shape)\n#print(y_train.shape,y_val.shape,y_test.shape)\n","metadata":{"papermill":{"duration":0.093957,"end_time":"2022-09-18T13:23:15.786210","exception":false,"start_time":"2022-09-18T13:23:15.692253","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T16:56:43.146211Z","iopub.execute_input":"2024-04-08T16:56:43.147126Z","iopub.status.idle":"2024-04-08T16:56:43.155854Z","shell.execute_reply.started":"2024-04-08T16:56:43.147080Z","shell.execute_reply":"2024-04-08T16:56:43.154757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dff=pd.DataFrame()\n#dff[['img_path','classes_with_ctext','joint_label']]=df[['img_path','classes_with_ctext','joint_label']]\n#dff['joint_label'].replace('', np.nan, inplace=True)\n#dff=df.dropna()\n#dff.shape","metadata":{"papermill":{"duration":0.085258,"end_time":"2022-09-18T13:23:16.266009","exception":false,"start_time":"2022-09-18T13:23:16.180751","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#obj_all=pd.read_csv('/gdrive/MyDrive/sumana_nuig/simpson one objectall info1.csv')\n#df=pd.read_csv('../input/mvsa1objectarray/mvsa_one_object925.csv')\n#obj_all[:2]\n#obj_all.shape\n#obj_all.jlabel.value_counts()\n#jlabel=pd.get_dummies(df.jsentiment)\n#vlabel=pd.get_dummies(df.vsentiment)\n#img_arr=np.load('/gdrive/MyDrive/sumana_nuig/simpson_global395_img.npy')\n#obj_arr=np.load('/gdrive/MyDrive/sumana_nuig/simpson_one_object395_img.npy')\n#img_arr=np.load('../input/mvsa1objectarray/mvsa_global_oneobject_img.npy')\n#obj_arr=np.load('../input/mvsa1objectarray/mvsa_oneobject_img.npy')\n#print(img_arr.shape)\n#print(obj_arr.shape)","metadata":{"papermill":{"duration":0.085711,"end_time":"2022-09-18T13:23:16.595236","exception":false,"start_time":"2022-09-18T13:23:16.509525","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_df[:2]","metadata":{"papermill":{"duration":0.086911,"end_time":"2022-09-18T13:23:16.925728","exception":false,"start_time":"2022-09-18T13:23:16.838817","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_label_code(text):\n    if text=='positive':\n        return 2\n    elif text=='neutral':\n        return 1\n    else :\n        return 0\ntest_df['label']=test_df['image_label'].apply(get_label_code)\n#test_df['jlabel']=test_df['joint_sentiment'].apply(get_label_code)\ntest_df['ilabel']=test_df['af'].apply(get_label_code)\n#test_df['vlabel']=test_df['vsentiment'].apply(get_label_code)\n#test_df['tlabel']=test_df['tsentiment'].apply(get_label_code)\n#test_df[:2]","metadata":{"papermill":{"duration":0.091999,"end_time":"2022-09-18T13:23:17.094286","exception":false,"start_time":"2022-09-18T13:23:17.002287","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T16:57:09.113877Z","iopub.execute_input":"2024-04-08T16:57:09.114279Z","iopub.status.idle":"2024-04-08T16:57:09.123882Z","shell.execute_reply.started":"2024-04-08T16:57:09.114242Z","shell.execute_reply":"2024-04-08T16:57:09.122826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''def get_label_code(text):\n    if text=='Positive':\n        return 2\n    elif text=='Neutral':\n        return 1\n    else :\n        return 0\n#test_df['jlabel']=test_df['joint_label'].apply(get_label_code)\ntest_df['jlabel']=test_df['joint_sentiment'].apply(get_label_code)\n#test_df['jlabel']=test_df['afc'].apply(get_label_code)'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Filtering all the warnings sign","metadata":{"papermill":{"duration":0.123153,"end_time":"2022-09-18T13:23:17.295366","exception":false,"start_time":"2022-09-18T13:23:17.172213","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n%config Completer.use_jedi = False # if autocompletion doesnot work in kaggle notebook | hit tab","metadata":{"papermill":{"duration":0.094277,"end_time":"2022-09-18T13:23:17.467697","exception":false,"start_time":"2022-09-18T13:23:17.373420","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Glob tokenizer\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.optimizers import Adam\nfrom keras_preprocessing.text import Tokenizer\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\nfrom keras_preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Flatten,Dropout,GlobalMaxPool1D,MaxPooling1D,Concatenate\nfrom tensorflow.keras.layers import SpatialDropout1D","metadata":{"execution":{"iopub.status.busy":"2024-04-08T13:20:39.325797Z","iopub.execute_input":"2024-04-08T13:20:39.326207Z","iopub.status.idle":"2024-04-08T13:20:39.332955Z","shell.execute_reply.started":"2024-04-08T13:20:39.326171Z","shell.execute_reply":"2024-04-08T13:20:39.331816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"max len of tweets\",max([len(x.split()) for x in train_df.cbts1]))","metadata":{"execution":{"iopub.status.busy":"2024-02-26T13:35:13.168219Z","iopub.execute_input":"2024-02-26T13:35:13.168632Z","iopub.status.idle":"2024-02-26T13:35:13.185109Z","shell.execute_reply.started":"2024-02-26T13:35:13.168597Z","shell.execute_reply":"2024-02-26T13:35:13.184102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GLOVE_EMB = '../input/glove6b300dtxt/glove.6B.300d.txt'\nEMBEDDING_DIM = 300","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:18:55.752748Z","iopub.execute_input":"2024-04-08T14:18:55.753530Z","iopub.status.idle":"2024-04-08T14:18:55.757844Z","shell.execute_reply.started":"2024-04-08T14:18:55.753490Z","shell.execute_reply":"2024-04-08T14:18:55.756707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Funcvtion for Vocab size and embedding vector**","metadata":{}},{"cell_type":"code","source":"#Training and Test\ndef load_tarin_test_dataset(train,test,col,str_len):\n    column=col\n    max_len=str_len\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts( train[column])\n    #tokenizer.fit_on_texts(train_df.object_attr1_text)\n    #tokenizer.fit_on_texts(train_df.text)\n    #tokenizer1.fit_on_texts(test_data)\n\n    word_index = tokenizer.word_index\n    vocab_size = len(tokenizer.word_index) + 1\n    #print(\"Vocabulary Size :\", vocab_size)\n\n    #jx_train = pad_sequences(tokenizer.texts_to_sequences(train_df.text), maxlen = max_len)\n    x_train = pad_sequences(tokenizer.texts_to_sequences( train[column]), maxlen = max_len)\n    #x_val = pad_sequences(tokenizer.texts_to_sequences(val_df.class_att), maxlen = 32)\n    #jx_test = pad_sequences(tokenizer.texts_to_sequences(test_df.text), maxlen =max_len )\n    x_test = pad_sequences(tokenizer.texts_to_sequences( test[column]), maxlen =max_len )\n    #y_train, y_test = train_test_split(y, test_size=1-TRAIN_SIZE,random_state=7) # Splits Dataset into Training and Testing set\n    #y_train, y_test = train_test_split(df['POSITIVE'], test_size=1-TRAIN_SIZE,random_state=7)\n    embeddings_index = {}\n\n    f = open(GLOVE_EMB, encoding=\"utf8\")\n    for line in f :\n        values = line.split()\n        word = value = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n    f.close()\n\n    print('Found %s word vectors.' %len(embeddings_index))\n    embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n    print(embedding_matrix.shape)\n    for word, i in word_index.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n\n    embedding_layer = tf.keras.layers.Embedding(vocab_size,\n                                          EMBEDDING_DIM,\n                                          weights=[embedding_matrix],\n                                          input_length=max_len,#MAX_SEQUENCE_LENGTH,\n                                          trainable=False)\n    return x_train,x_test,vocab_size,embedding_layer\n","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:18:58.235059Z","iopub.execute_input":"2024-04-08T14:18:58.236043Z","iopub.status.idle":"2024-04-08T14:18:58.248321Z","shell.execute_reply.started":"2024-04-08T14:18:58.236003Z","shell.execute_reply":"2024-04-08T14:18:58.247412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#For class-attributes pairs\n\n#call train test dataset\nmax_len=max([len(x.split()) for x in train_df.class_att_res50])\n#tx_train,tx_test,vocab_size,embedding_layer=load_tarin_test_dataset(train_df,test_df,'text',max([len(x.split()) for x in train_df.text]))\nax_train,ax_test,vocab_size,embedding_layer=load_tarin_test_dataset(train_df,test_df,'class_att_res50',max_len)\nprint(\"Vocabulary Size :\", vocab_size)\nprint(\"Training X Shape:\",ax_train.shape)\nprint(\"Testing X Shape:\",ax_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:23:47.011371Z","iopub.execute_input":"2024-04-08T14:23:47.012138Z","iopub.status.idle":"2024-04-08T14:24:17.930400Z","shell.execute_reply.started":"2024-04-08T14:23:47.012099Z","shell.execute_reply":"2024-04-08T14:24:17.929380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cnn_model(num_filters,kernel_size,string_length,embeding_layer):\n    num_filters=num_filters\n    kernel_size=kernel_size\n    #jsequence_input = Input(shape=(maxlen,), dtype='int32')\n    sequence_input = Input(shape=(max_len,), dtype='int32')\n    embedding_sequences = embedding_layer(sequence_input)\n    x = SpatialDropout1D(0.2)(embedding_sequences)\n    conv_layers = []\n    for num_filters,kernel_size in zip(num_filters,kernel_size):\n        conv_layer = Conv1D(filters=num_filters, kernel_size=kernel_size, activation='relu')(x)#(jembedding_sequences)\n        pool_layer = MaxPooling1D(pool_size=max_len - kernel_size + 1)(conv_layer)\n        #pool_layer = MaxPooling1D()(conv_layer)\n        #pool_layer = tf.keras.layers.Reshape((1, -1))(pool_layer)\n        conv_layers.append(pool_layer)\n\n    #Concatenate all convolutional layers\n    concatenate_layer = tf.keras.layers.Concatenate()(conv_layers)\n    flatten_layer = Flatten()(concatenate_layer)\n    x = Dropout(0.5)(flatten_layer)#(x)\n    txt = Dense(256, activation='relu')(x)\n\n    #x = Dense(100, kernel_regularizer=tf.keras.regularizers.l2(0.01), activation='relu')(x)\n    #x = Dropout(0.5)(x)\n    #outputs = Dense(3, kernel_regularizer=tf.keras.regularizers.l2(0.01),activation='softmax')(x)\n    #outputs = Dense(3,activation='softmax')(x)\n    #model= tf.keras.Model(jsequence_input, outputs)\n    #return model\n    return sequence_input,txt\n#modelse=cnn_model(num_filters,kernel_sizes,max_len,embedding_layer)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T18:44:24.649507Z","iopub.execute_input":"2024-02-20T18:44:24.650258Z","iopub.status.idle":"2024-02-20T18:44:24.659294Z","shell.execute_reply.started":"2024-02-20T18:44:24.650219Z","shell.execute_reply":"2024-02-20T18:44:24.658347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#For class-attributes pairs\n\n#call train test dataset\nmax_len=max([len(x.split()) for x in train_df.class_att1])\n#tx_train,tx_test,vocab_size,embedding_layer=load_tarin_test_dataset(train_df,test_df,'text',max([len(x.split()) for x in train_df.text]))\nax_train,ax_test,vocab_size,embedding_layer=load_tarin_test_dataset(train_df,test_df,'class_att1',max_len)\nprint(\"Vocabulary Size :\", vocab_size)\nprint(\"Training X Shape:\",ax_train.shape)\nprint(\"Testing X Shape:\",ax_test.shape)\n#call CNN model\nkernel_sizes = [4,3]\nnum_filters = [32,16]\natclass_input,tcl=cnn_model(num_filters,kernel_sizes,max_len,embedding_layer)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T18:44:45.372570Z","iopub.execute_input":"2024-02-20T18:44:45.373508Z","iopub.status.idle":"2024-02-20T18:45:22.584066Z","shell.execute_reply.started":"2024-02-20T18:44:45.373453Z","shell.execute_reply":"2024-02-20T18:45:22.582919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#For class-attributes pairs\n\n#call train test dataset\nmax_len=max([len(x.split()) for x in train_df.scene_res50])\n#tx_train,tx_test,vocab_size,embedding_layer=load_tarin_test_dataset(train_df,test_df,'text',max([len(x.split()) for x in train_df.text]))\nsx_train,sx_test,vocab_size,embedding_layer=load_tarin_test_dataset(train_df,test_df,'scene_res50',max_len)\nprint(\"Vocabulary Size :\", vocab_size)\nprint(\"Training X Shape:\",sx_train.shape)\nprint(\"Testing X Shape:\",sx_test.shape)\n#call CNN model\nkernel_sizes = [4,3]\nnum_filters = [32,16]\nscene_input,scene=cnn_model(num_filters,kernel_sizes,max_len,embedding_layer)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:19:29.911377Z","iopub.execute_input":"2024-04-08T14:19:29.911831Z","iopub.status.idle":"2024-04-08T14:20:00.960646Z","shell.execute_reply.started":"2024-04-08T14:19:29.911795Z","shell.execute_reply":"2024-04-08T14:20:00.959225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#For class-attributes pairs\n\n#call train test dataset\nmax_len=max([len(x.split()) for x in train_df.class_att_res50])\n#tx_train,tx_test,vocab_size,embedding_layer=load_tarin_test_dataset(train_df,test_df,'text',max([len(x.split()) for x in train_df.text]))\njx_train,jx_test,vocab_size,embedding_layer=load_tarin_test_dataset(train_df,test_df,'class_att_res50',max_len)\nprint(\"Vocabulary Size :\", vocab_size)\nprint(\"Training X Shape:\",jx_train.shape)\nprint(\"Testing X Shape:\",jx_test.shape)\n#call CNN model\nkernel_sizes = [5,4]\nnum_filters = [64,32]\nca_scene_input,ca_scene=cnn_model(num_filters,kernel_sizes,max_len,embedding_layer)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T19:13:45.863477Z","iopub.execute_input":"2024-02-20T19:13:45.863894Z","iopub.status.idle":"2024-02-20T19:14:17.964865Z","shell.execute_reply.started":"2024-02-20T19:13:45.863860Z","shell.execute_reply":"2024-02-20T19:14:17.963692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rcl = tf.keras.layers.Reshape((1, -1))(tcl)\nrttxt = tf.keras.layers.Reshape((1, -1))(ca_scene)\n# Combine class-att and caption text pathways using attention mechanism\nattention1 = tf.keras.layers.AdditiveAttention()([rcl, rttxt])\n#attention1 = tf.keras.layers.GlobalMaxPool1D()(attention1)\n#frcl=Flatten()(rcl)\nfrttxt=Flatten()(rttxt)\nfattention1=Flatten()(attention1)\nat1 =  tf.keras.layers.Concatenate()([frttxt, fattention1])\nat1 = Dense(256, activation='relu')(at1)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T19:46:58.297528Z","iopub.execute_input":"2024-02-20T19:46:58.297938Z","iopub.status.idle":"2024-02-20T19:46:58.345504Z","shell.execute_reply.started":"2024-02-20T19:46:58.297901Z","shell.execute_reply":"2024-02-20T19:46:58.344656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfat1=Flatten()(at1)\n#fca_scene=Flatten()(ca_scene)\nfusion2 = tf.keras.layers.Concatenate()([fat1,scene,ca_scene])\n#fusion2 = tf.keras.layers.Concatenate()([tcl,scene,ca_scene])\nfusion2 = Dense(256,kernel_regularizer=tf.keras.regularizers.l2(0.01),activation='relu')(fusion2)#(att_input_layer)\nfusion2 = Dropout(0.5)(fusion2)\nfusion2_out = Dense(3,kernel_regularizer=tf.keras.regularizers.l2(0.01),activation='softmax')(fusion2)\n# Defining model input and output\nf2model = tf.keras.Model(inputs = [atclass_input,scene_input,ca_scene_input], outputs=fusion2_out)\n# Compile the model\nf2model.compile(\n    optimizer =tf.keras.optimizers.Adam(learning_rate=0.01),# optimizer,#image_input\n    loss = 'categorical_crossentropy',#loss\n    metrics =['accuracy'])\n# metric)#Max length 55 with ctext with class name with joint label\nf2history = f2model.fit(\n    x =[ax_train,sx_train,jx_train],\n    y = y_train,#to_categorical(train_df.tlabel),\n    #validation_data = ([vx_test,\n    #{'input_ids':tx_test['input_ids'],'attention_mask':tx_test['attention_mask']}],y_test #to_categorical(test_df.tlabel)\n   # ),\n validation_split = 0.1,\n  epochs=10,\n    batch_size=32\n)\n\n#text classification without english stop word and max word length 80\nf2predicted = f2model.predict([ax_test,sx_test,jx_test])\n#com_score=f2model.evaluate([ax_test,sx_test,jx_test],y_test)\n#print('Test Loss:', com_score[0])\n#print('Test accuracy:', com_score[1])\nypredicted = np.argmax(f2predicted,axis=1)\n#y_predicted = np.where(predicted>0.5,1,0)\nfrom sklearn.metrics import classification_report\nprint(classification_report(test_df.ilabel,ypredicted))\n\n#accuracy vs loss function\ns, (at, al) = plt.subplots(2,1)\nat.plot(f2history.history['accuracy'], c= 'b')\nat.plot(f2history.history['val_accuracy'], c='r')\nat.set_title('model accuracy')\nat.set_ylabel('accuracy')\nat.set_xlabel('epoch')\nat.legend(['Res_bilstm_train', 'Res_bilstm_val'], loc='upper left')\n\nal.plot(f2history.history['loss'], c='m')\nal.plot(f2history.history['val_loss'], c='c')\nal.set_title('model loss')\nal.set_ylabel('loss')\nal.set_xlabel('epoch')\nal.legend(['train', 'val'], loc = 'upper left')\n#ypredicted = np.argmax(f2predicted,axis=1)\nbal_acc=balanced_accuracy_score(test_df.ilabel,ypredicted)\nrecall=recall_score(test_df.ilabel,ypredicted,average='weighted')\nprecision=precision_score(test_df.ilabel,ypredicted,average='weighted')\nf1=f1_score(test_df.ilabel,ypredicted,average='weighted')\nprint(acc,bal_acc,recall,precision,f1)","metadata":{"execution":{"iopub.status.busy":"2024-02-14T01:00:43.609895Z","iopub.execute_input":"2024-02-14T01:00:43.610365Z","iopub.status.idle":"2024-02-14T01:00:52.193579Z","shell.execute_reply.started":"2024-02-14T01:00:43.610329Z","shell.execute_reply":"2024-02-14T01:00:52.192515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nypredicted = np.argmax(f2predicted,axis=1)\nprint(classification_report(test_df.ilabel,ypredicted,digits=4))\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nacc=accuracy_score(test_df.ilabel,ypredicted)\nbal_acc=balanced_accuracy_score(test_df.ilabel,ypredicted)\nrecall=recall_score(test_df.ilabel,ypredicted,average='weighted')\nprecision=precision_score(test_df.ilabel,ypredicted,average='weighted')\nf1=f1_score(test_df.ilabel,ypredicted,average='weighted')\nprint(acc,bal_acc,recall,precision,f1)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T23:04:18.162522Z","iopub.execute_input":"2024-02-13T23:04:18.162971Z","iopub.status.idle":"2024-02-13T23:04:18.184879Z","shell.execute_reply.started":"2024-02-13T23:04:18.162934Z","shell.execute_reply":"2024-02-13T23:04:18.183744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fat1=Flatten()(at1)\n#fca_scene=Flatten()(ca_scene)\nfusion2 = tf.keras.layers.Concatenate()([fat1,tcl,scene,ca_scene])\n#fusion2 = tf.keras.layers.Concatenate()([fat1,ca_scene])\nfusion2 = Dense(256,activation='relu')(fusion2)#(att_input_layer)\nfusion2 = Dropout(0.5)(fusion2)\nfusion2_out = Dense(3,kernel_regularizer=tf.keras.regularizers.l2(0.01),activation='softmax')(fusion2)\n# Defining model input and output\nf2model = tf.keras.Model(inputs = [atclass_input,scene_input,ca_scene_input], outputs=fusion2_out)\n# Compile the model\nf2model.compile(\n    optimizer =tf.keras.optimizers.Adam(learning_rate=0.01),# optimizer,#image_input\n    loss = 'categorical_crossentropy',#loss\n    metrics =['accuracy'])\n# metric)#Max length 55 with ctext with class name with joint label\nf2history = f2model.fit(\n    x =[ax_train,sx_train,jx_train],\n    y = y_train,#to_categorical(train_df.tlabel),\n    #validation_data = ([vx_test,\n    #{'input_ids':tx_test['input_ids'],'attention_mask':tx_test['attention_mask']}],y_test #to_categorical(test_df.tlabel)\n   # ),\n validation_split = 0.1,\n  epochs=10,\n    batch_size=32\n)\n\n#text classification without english stop word and max word length 80\nf2predicted = f2model.predict([ax_test,sx_test,jx_test])\n#com_score=f2model.evaluate([ax_test,sx_test,jx_test],y_test)\n#print('Test Loss:', com_score[0])\n#print('Test accuracy:', com_score[1])\nypredicted = np.argmax(f2predicted,axis=1)\n#y_predicted = np.where(predicted>0.5,1,0)\nfrom sklearn.metrics import classification_report\nprint(classification_report(test_df.ilabel,ypredicted))\n\n#accuracy vs loss function\ns, (at, al) = plt.subplots(2,1)\nat.plot(f2history.history['accuracy'], c= 'b')\nat.plot(f2history.history['val_accuracy'], c='r')\nat.set_title('model accuracy')\nat.set_ylabel('accuracy')\nat.set_xlabel('epoch')\nat.legend(['Res_bilstm_train', 'Res_bilstm_val'], loc='upper left')\n\nal.plot(f2history.history['loss'], c='m')\nal.plot(f2history.history['val_loss'], c='c')\nal.set_title('model loss')\nal.set_ylabel('loss')\nal.set_xlabel('epoch')\nal.legend(['train', 'val'], loc = 'upper left')\n#ypredicted = np.argmax(f2predicted,axis=1)\nbal_acc=balanced_accuracy_score(test_df.ilabel,ypredicted)\nrecall=recall_score(test_df.ilabel,ypredicted,average='weighted')\nprecision=precision_score(test_df.ilabel,ypredicted,average='weighted')\nf1=f1_score(test_df.ilabel,ypredicted,average='weighted')\nprint(acc,bal_acc,recall,precision,f1)","metadata":{"execution":{"iopub.status.busy":"2024-02-14T01:20:40.550762Z","iopub.execute_input":"2024-02-14T01:20:40.551193Z","iopub.status.idle":"2024-02-14T01:20:49.100483Z","shell.execute_reply.started":"2024-02-14T01:20:40.551156Z","shell.execute_reply":"2024-02-14T01:20:49.099407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#class_attributes  tokenization\nmax_len=80\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_df.text)\n#tokenizer.fit_on_texts(train_df.object_attr1_text)\n#tokenizer1.fit_on_texts(test_data)\n\nword_index = tokenizer.word_index\njvocab_size = len(tokenizer.word_index) + 1\n    #print(\"Vocabulary Size :\", vocab_size)\n\njx_train = pad_sequences(tokenizer.texts_to_sequences(train_df.text), maxlen = max_len)\n#jx_train = pad_sequences(tokenizer.texts_to_sequences(train_df.object_attr1_text), maxlen = max_len)\n#x_val = pad_sequences(tokenizer.texts_to_sequences(val_df.class_att), maxlen = 32)\njx_test = pad_sequences(tokenizer.texts_to_sequences(test_df.text), maxlen =max_len )\n#jx_test = pad_sequences(tokenizer.texts_to_sequences(test_df.object_attr1_text), maxlen =max_len )\n#y_train, y_test = train_test_split(y, test_size=1-TRAIN_SIZE,random_state=7) # Splits Dataset into Training and Testing set \n#y_train, y_test = train_test_split(df['POSITIVE'], test_size=1-TRAIN_SIZE,random_state=7)\njembeddings_index = {}\n\nf = open(GLOVE_EMB, encoding=\"utf8\")\nfor line in f :\n    values = line.split()\n    word = value = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    jembeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' %len(jembeddings_index))\njembedding_matrix = np.zeros((jvocab_size, EMBEDDING_DIM))\nprint(jembedding_matrix.shape)\nfor word, i in word_index.items():\n    embedding_vector = jembeddings_index.get(word)\n    if embedding_vector is not None:\n        jembedding_matrix[i] = embedding_vector\n    \njembedding_layer = tf.keras.layers.Embedding(jvocab_size,\n                                          EMBEDDING_DIM,\n                                          weights=[jembedding_matrix],\n                                          input_length=max_len,#MAX_SEQUENCE_LENGTH,\n                                          trainable=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#class_attributes  tokenization\nmax_len=80#40\ntokenizer = Tokenizer()\n#tokenizer.fit_on_texts(train_df.class_att_res50)\n#tokenizer.fit_on_texts(train_df.class_att)\ntokenizer.fit_on_texts(train_df.cbts1)\n#tokenizer1.fit_on_texts(test_data)\n\nword_index = tokenizer.word_index\njvocab_size = len(tokenizer.word_index) + 1\n    #print(\"Vocabulary Size :\", vocab_size)\n\n#jx_train = pad_sequences(tokenizer.texts_to_sequences(train_df.class_att_res50), maxlen = max_len)\njx_train = pad_sequences(tokenizer.texts_to_sequences(train_df.cbts1), maxlen = max_len)\n#jx_train = pad_sequences(tokenizer.texts_to_sequences(train_df.class_att1), maxlen = max_len)\n\n#jx_test = pad_sequences(tokenizer.texts_to_sequences(test_df.class_att_res50), maxlen =max_len )\njx_test = pad_sequences(tokenizer.texts_to_sequences(test_df.cbts1), maxlen =max_len )\n#jx_test = pad_sequences(tokenizer.texts_to_sequences(test_df.class_att1), maxlen =max_len )\n#y_train, y_test = train_test_split(y, test_size=1-TRAIN_SIZE,random_state=7) # Splits Dataset into Training and Testing set\n#y_train, y_test = train_test_split(df['POSITIVE'], test_size=1-TRAIN_SIZE,random_state=7)\njembeddings_index = {}\n\nf = open(GLOVE_EMB, encoding=\"utf8\")\nfor line in f :\n    values = line.split()\n    word = value = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    jembeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' %len(jembeddings_index))\njembedding_matrix = np.zeros((jvocab_size, EMBEDDING_DIM))\nprint(jembedding_matrix.shape)\nfor word, i in word_index.items():\n    embedding_vector = jembeddings_index.get(word)\n    if embedding_vector is not None:\n        jembedding_matrix[i] = embedding_vector\n\njembedding_layer = tf.keras.layers.Embedding(jvocab_size,\n                                          EMBEDDING_DIM,\n                                          weights=[jembedding_matrix],\n                                          input_length=max_len,#MAX_SEQUENCE_LENGTH,\n                                          trainable=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:18:25.122360Z","iopub.execute_input":"2024-04-08T14:18:25.122787Z","iopub.status.idle":"2024-04-08T14:18:25.161339Z","shell.execute_reply.started":"2024-04-08T14:18:25.122753Z","shell.execute_reply":"2024-04-08T14:18:25.160112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jsequence_input = Input(shape=(max_len,), dtype='int32')\njembedding_sequences = embedding_layer(jsequence_input)\nx = SpatialDropout1D(0.2)(jembedding_sequences)\n#x = Conv1D(64, 5, kernel_regularizer=tf.keras.regularizers.l2(0.001),activation='relu')(x)\nx = Conv1D(32, 5,activation='relu')(x)\n#x=Conv1D(64, 5,padding='same', kernel_initializer='he_uniform',activation='relu')(x)#(jembedding_sequences)#\n\nx= GlobalMaxPool1D()(x)\nx = Dropout(0.5)(x)\n#x=Flatten()(x)\n#x = Dense(10, activation='relu')(x)\n#outputs = Dense(3, kernel_regularizer=tf.keras.regularizers.l2(0.01),activation='softmax')(x)\noutputs = Dense(3,activation='softmax')(x)\nmodelse= tf.keras.Model(jsequence_input, outputs)\n#Model Compile\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nmodelse.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy',\n              metrics=['categorical_accuracy'])\n\nhistoryse = modelse.fit(ax_train, y_train, epochs=70,batch_size=32, validation_split=0.1,validation_data=(ax_test, y_test))\n\ns, (at, al) = plt.subplots(2,1)\nat.plot(historyse.history['categorical_accuracy'], c= 'b')\nat.plot(historyse.history['val_categorical_accuracy'], c='r')\nat.set_title('model accuracy')\nat.set_ylabel('accuracy')\nat.set_xlabel('epoch')\nat.legend(['CNN_train', 'CNN_val'], loc='upper left')\n\nal.plot(historyse.history['loss'], c='m')\nal.plot(historyse.history['val_loss'], c='c')\nal.set_title('model loss')\nal.set_ylabel('loss')\nal.set_xlabel('epoch')\nal.legend(['train', 'val'], loc = 'upper left')\n#<matplotlib.legend.Legend at 0x7f3739c12490>\n#ypred=modelse.predict(x_test)\n#ypred=np.around(ypred)\npredicted = modelse.predict(ax_test)\n#bscore=modelse.evaluate(jx_test,y_test)\n#print('Test Loss:', bscore[0])\n#print('Test accuracy:', bscore[1])\nypredicted = np.argmax(predicted,axis=1)\n#y_predicted = np.where(predicted>0.5,1,0)\nfrom sklearn.metrics import classification_report\n#print('Precision_Score  %:',(precision_score(y_test, ypred, average ='macro'))*100)\n#print('F1_Score         %:',(f1_score(y_test,ypred, average='macro'))*100)\n#print('Recall_Score     %:',(recall_score(y_test, ypred, average='macro'))*100)\nprint(classification_report(test_df.ilabel,ypredicted,digits=4))\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nacc=accuracy_score(test_df.ilabel,ypredicted)\nbal_acc=balanced_accuracy_score(test_df.ilabel,ypredicted)\nrecall=recall_score(test_df.ilabel,ypredicted,average='weighted')\nprecision=precision_score(test_df.ilabel,ypredicted,average='weighted')\nf1=f1_score(test_df.ilabel,ypredicted,average='weighted')\nprint(acc,bal_acc,recall,precision,f1)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:25:29.007652Z","iopub.execute_input":"2024-04-08T14:25:29.008061Z","iopub.status.idle":"2024-04-08T14:26:10.830520Z","shell.execute_reply.started":"2024-04-08T14:25:29.008025Z","shell.execute_reply":"2024-04-08T14:26:10.829558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jsequence_input = Input(shape=(max_len,), dtype='int32')\njembedding_sequences = jembedding_layer(jsequence_input)\nx = SpatialDropout1D(0.2)(jembedding_sequences)\nkernel_size = [5,4,3]\nnum_filters = [64,32,16]\nconv_layers = []\nfor num_filters,kernel_size in zip(num_filters,kernel_size):\n    conv_layer = Conv1D(filters=num_filters, kernel_size=kernel_size, activation='relu')(x)#(jembedding_sequences)\n    pool_layer = MaxPooling1D(pool_size=max_len - kernel_size + 1)(conv_layer)\n    #pool_layer = MaxPooling1D()(conv_layer)\n    #pool_layer = tf.keras.layers.Reshape((1, -1))(pool_layer)\n    conv_layers.append(pool_layer)\n\n#Concatenate all convolutional layers\nconcatenate_layer = tf.keras.layers.Concatenate()(conv_layers)\nflatten_layer = Flatten()(concatenate_layer)\nx = Dropout(0.5)(flatten_layer)#(x)\n#x = Conv1D(128, 5, kernel_regularizer=tf.keras.regularizers.l2(0.001),activation='relu')(x)\n#x = Conv1D(64, 5,activation='relu')(x)\n#x=Conv1D(64, 5,padding='same', kernel_initializer='he_uniform',activation='relu')(x)#(jembedding_sequences)#\n\n#x= GlobalMaxPool1D()(x)\n#x = Dropout(0.5)(x)\n#x=Flatten()(x)\n#x = Dense(10, activation='relu')(x)\noutputs = Dense(3, kernel_regularizer=tf.keras.regularizers.l2(0.01),activation='softmax')(x)\n#outputs = Dense(3,activation='softmax')(x)\nmodelse= tf.keras.Model(jsequence_input, outputs)\n#Model Compile\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nmodelse.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy',\n              metrics=['categorical_accuracy'])\n\nhistoryse = modelse.fit(jx_train, y_train, epochs=100,batch_size=32, validation_split=0.1,validation_data=(jx_test, y_test))\n\ns, (at, al) = plt.subplots(2,1)\nat.plot(historyse.history['categorical_accuracy'], c= 'b')\nat.plot(historyse.history['val_categorical_accuracy'], c='r')\nat.set_title('model accuracy')\nat.set_ylabel('accuracy')\nat.set_xlabel('epoch')\nat.legend(['CNN_train', 'CNN_val'], loc='upper left')\n\nal.plot(historyse.history['loss'], c='m')\nal.plot(historyse.history['val_loss'], c='c')\nal.set_title('model loss')\nal.set_ylabel('loss')\nal.set_xlabel('epoch')\nal.legend(['train', 'val'], loc = 'upper left')\n#<matplotlib.legend.Legend at 0x7f3739c12490>\n#ypred=modelse.predict(x_test)\n#ypred=np.around(ypred)\npredicted = modelse.predict(jx_test)\nbscore=modelse.evaluate(jx_test,y_test)\nprint('Test Loss:', bscore[0])\nprint('Test accuracy:', bscore[1])\nypredicted = np.argmax(predicted,axis=1)\n#y_predicted = np.where(predicted>0.5,1,0)\nfrom sklearn.metrics import classification_report\nprint(classification_report(test_df.jlabel,ypredicted))\nbal_acc=balanced_accuracy_score(test_df.jlabel,ypredicted)\nrecall=recall_score(test_df.jlabel,ypredicted,average='weighted')\nprecision=precision_score(test_df.jlabel,ypredicted,average='weighted')\nf1=f1_score(test_df.jlabel,ypredicted,average='weighted')\nprint(acc,bal_acc,recall,precision,f1)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T10:04:32.544165Z","iopub.execute_input":"2024-02-26T10:04:32.545102Z","iopub.status.idle":"2024-02-26T10:04:56.700582Z","shell.execute_reply.started":"2024-02-26T10:04:32.545063Z","shell.execute_reply":"2024-02-26T10:04:56.699770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jsequence_input = Input(shape=(max_len,), dtype='int32')\njembedding_sequences = jembedding_layer(jsequence_input)\n#s = SpatialDropout1D(0.2)(embedding_sequences)\n#x = Conv1D(64, 5, activation='relu')(x)\n#x = Bidirectional(LSTM(200, dropout=0.2, recurrent_dropout=0.2,return_sequences=True))()\nbilstm = Bidirectional(LSTM(32,dropout=0.2, recurrent_dropout=0.2))((jembedding_sequences))\n#bilstm = Dense(256, activation='tanh')(bilstm)\nbilstm = Dense(1024, activation='relu')(bilstm)\nx = Dropout(0.2)(bilstm)#(x)\n#x = Dense(10, activation='relu')(x)\noutputs = Dense(3, kernel_regularizer=tf.keras.regularizers.l2(0.01),activation='softmax')(x)\n#outputs = Dense(3,activation='softmax')(x)\nmodelse= tf.keras.Model(jsequence_input, outputs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#For coattention\njsequence_input = Input(shape=(max_len,), dtype='int32')\njembedding_sequences = jembedding_layer(jsequence_input)\nx = SpatialDropout1D(0.1)(jembedding_sequences)\nconv_layers = []\n    for num_filters,kernel_size in zip(num_filters,kernel_size):\n        conv_layer = Conv1D(filters=num_filters, kernel_size=kernel_size, activation='relu')(x)#(jembedding_sequences)\n        pool_layer = MaxPooling1D(pool_size=max_len - kernel_size + 1)(conv_layer)\n        #pool_layer = MaxPooling1D()(conv_layer)\n        #pool_layer = tf.keras.layers.Reshape((1, -1))(pool_layer)\n        conv_layers.append(pool_layer)\n\n    #Concatenate all convolutional layers\n    concatenate_layer = tf.keras.layers.Concatenate()(conv_layers)\n    flatten_layer = Flatten()(concatenate_layer)\n    x = Dropout(0.5)(flatten_layer)#(x)\nx = Conv1D(256, 5, activation='relu')(x)\n#x=Conv1D(32, 5,padding='same',activation='relu')(x)\nx= GlobalMaxPool1D()(x)\ncn = Dense(256, activation='tanh')(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jsequence_input = Input(shape=(max_len,), dtype='int32')\njembedding_sequences = embedding_layer(jsequence_input)\nx = SpatialDropout1D(0.2)(jembedding_sequences)\n#x = Conv1D(64, 5, kernel_regularizer=tf.keras.regularizers.l2(0.01),activation='relu')(x)\n#cn = Dense(32, activation='tanh')(x)\n#x=Conv1D(32, 5,padding='same',activation='relu')(x)\n#x= GlobalMaxPool1D()(x)\n#x = Bidirectional(LSTM(200, dropout=0.2, recurrent_dropout=0.2,return_sequences=True))()\nx = Bidirectional(LSTM(32,dropout=0.2, recurrent_dropout=0.2,activation='relu'))(x)\n#((jembedding_sequences))\nx = Dense(62 ,activation='relu')(x)\n#x = Dense(32, kernel_regularizer=tf.keras.regularizers.l2(0.01),activation='relu')(x)\n#x = Conv1D(128, 3, activation='relu\n#y=Conv1D(200, 5,padding='same', kernel_initializer='he_uniform')(s)\n#y=Conv1D(150, 5,padding='same',activation='relu')(s)\n#y=Conv1D(100, 5,padding='same',activation='relu')(y)\n#y= GlobalMaxPool1D()(y)\n#y1=Conv1D(200, 5, padding='same', kernel_initializer='he_uniform', activation='relu')(s)\n#y1= GlobalMaxPool1D()(y1)\n#y2=Conv1D(64, 5, padding='same', kernel_initializer='he_uniform', activation='relu')(s)\n#x = LSTM(32, dropout=0.2, recurrent_dropout=0.2)(x)\n#y2= GlobalMaxPool1D()(y2)\n#z = Concatenate()([y, x])\n#z = Dense(256,activation='relu')(z)\n#z = Dense(256,kernel_regularizer=regularizers.l2(0.01),activation='relu')(z)\n#z = Dense(64, kernel_regularizer=regularizers.l2(0.01),activation='relu')(z)\n#x = Dense(10, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\nx = Dropout(0.5)(x)\n#x = Dense(10, activation='relu')(x)\noutputs = Dense(3, kernel_regularizer=tf.keras.regularizers.l2(0.01),activation='softmax')(x)\n#outputs = Dense(3,activation='softmax')(x)\nmodelse= tf.keras.Model(jsequence_input, outputs)\n#Model Compile\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nmodelse.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy',\n              metrics=['categorical_accuracy'])\n\nhistoryse = modelse.fit(ax_train, y_train, epochs=30,batch_size=32, validation_split=0.1,validation_data=(ax_test, y_test))\n\ns, (at, al) = plt.subplots(2,1)\nat.plot(historyse.history['categorical_accuracy'], c= 'b')\nat.plot(historyse.history['val_categorical_accuracy'], c='r')\nat.set_title('model accuracy')\nat.set_ylabel('accuracy')\nat.set_xlabel('epoch')\nat.legend(['BILSTM_train', 'BILSTM_val'], loc='upper left')\n\nal.plot(historyse.history['loss'], c='m')\nal.plot(historyse.history['val_loss'], c='c')\nal.set_title('model loss')\nal.set_ylabel('loss')\nal.set_xlabel('epoch')\nal.legend(['train', 'val'], loc = 'upper left')\n#<matplotlib.legend.Legend at 0x7f3739c12490>\n#ypred=modelse.predict(x_test)\n#ypred=np.around(ypred)\npredicted = modelse.predict(ax_test)\n#bscore=modelse.evaluate(jx_test,y_test)\n#print('Test Loss:', bscore[0])\n#print('Test accuracy:', bscore[1])\nypredicted = np.argmax(predicted,axis=1)\n#y_predicted = np.where(predicted>0.5,1,0)\nfrom sklearn.metrics import classification_report\nprint(classification_report(test_df.ilabel,ypredicted))\n#from sklearn.metrics import accuracy_score,jaccard_score,precision_score,recall_score,f1_score\n#print(\"Accuracy = \",(accuracy_score(y_test,ypred, normalize=True, sample_weight=None)*100))\n#print('Jaccard Accuracy %:',(jaccard_score(y_test,ypred, average='macro'))*100)\n#print('Precision_Score  %:',(precision_score(y_test, ypred, average ='macro'))*100)\n#print('F1_Score         %:',(f1_score(y_test,ypred, average='macro'))*100)\n#print('Recall_Score     %:',(recall_score(y_test, ypred, average='macro'))*100)\n#ypredicted = np.argmax(predicted,axis=1)\n#y_predicted = np.where(predicted>0.5,1,0)\n#from sklearn.metrics import classification_report\n#print(classification_report(test_df.jlabel,ypredicted,digits=4))\nbal_acc=balanced_accuracy_score(test_df.ilabel,ypredicted)\nrecall=recall_score(test_df.ilabel,ypredicted,average='weighted')\nprecision=precision_score(test_df.ilabel,ypredicted,average='weighted')\nf1=f1_score(test_df.ilabel,ypredicted,average='weighted')\nprint(acc,bal_acc,recall,precision,f1)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:32:15.660566Z","iopub.execute_input":"2024-04-08T14:32:15.661016Z","iopub.status.idle":"2024-04-08T14:54:37.793805Z","shell.execute_reply.started":"2024-04-08T14:32:15.660983Z","shell.execute_reply":"2024-04-08T14:54:37.792780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted = modelse.predict(ax_test)\n#bscore=modelse.evaluate(jx_test,y_test)\n#print('Test Loss:', bscore[0])\n#print('Test accuracy:', bscore[1])\nypredicted = np.argmax(predicted,axis=1)\n#y_predicted = np.where(predicted>0.5,1,0)\nfrom sklearn.metrics import classification_report\nprint(classification_report(test_df.ilabel,ypredicted,digits=4))\n#from sklearn.metrics import accuracy_score,jaccard_score,precision_score,recall_score,f1_score\n#print(\"Accuracy = \",(accuracy_score(y_test,ypred, normalize=True, sample_weight=None)*100))\n#print('Jaccard Accuracy %:',(jaccard_score(y_test,ypred, average='macro'))*100)\n#print('Precision_Score  %:',(precision_score(y_test, ypred, average ='macro'))*100)\n#print('F1_Score         %:',(f1_score(y_test,ypred, average='macro'))*100)\n#print('Recall_Score     %:',(recall_score(y_test, ypred, average='macro'))*100)\n#ypredicted = np.argmax(predicted,axis=1)\n#y_predicted = np.where(predicted>0.5,1,0)\n#from sklearn.metrics import classification_report\n#print(classification_report(test_df.jlabel,ypredicted,digits=4))\nacc=accuracy_score(test_df.ilabel,ypredicted)\nbal_acc=balanced_accuracy_score(test_df.ilabel,ypredicted)\nrecall=recall_score(test_df.ilabel,ypredicted,average='weighted')\nprecision=precision_score(test_df.ilabel,ypredicted,average='weighted')\nf1=f1_score(test_df.ilabel,ypredicted,average='weighted')\nprint(acc,bal_acc,recall,precision,f1)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-08T15:05:30.452049Z","iopub.execute_input":"2024-04-08T15:05:30.453242Z","iopub.status.idle":"2024-04-08T15:05:31.414267Z","shell.execute_reply.started":"2024-04-08T15:05:30.453189Z","shell.execute_reply":"2024-04-08T15:05:31.413211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import balanced_accuracy_score\nfrom sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nypredicted = np.argmax(predicted,axis=1)\nprint(classification_report(test_df.ilabel,ypredicted,digits=4))\nacc=accuracy_score(test_df.ilabel,ypredicted)\nbal_acc=balanced_accuracy_score(test_df.ilabel,ypredicted)\nrecall=recall_score(test_df.ilabel,ypredicted,average='weighted')\nprecision=precision_score(test_df.ilabel,ypredicted,average='weighted')\nf1=f1_score(test_df.ilabel,ypredicted,average='weighted')\nprint(acc,bal_acc,recall,precision,f1)","metadata":{"execution":{"iopub.status.busy":"2024-02-12T21:20:16.356425Z","iopub.execute_input":"2024-02-12T21:20:16.357198Z","iopub.status.idle":"2024-02-12T21:20:16.377355Z","shell.execute_reply.started":"2024-02-12T21:20:16.357160Z","shell.execute_reply":"2024-02-12T21:20:16.376381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jsequence_input = Input(shape=(max_len,), dtype='int32')\njembedding_sequences = jembedding_layer(jsequence_input)\nx = SpatialDropout1D(0.2)(jembedding_sequences)\n#x = Conv1D(128, 5, kernel_regularizer=tf.keras.regularizers.l2(0.001),activation='relu')(x)\nx = Conv1D(64, 5,activation='relu')(x)\n#x=Conv1D(64, 5,padding='same', kernel_initializer='he_uniform')(x)\n#x=Conv1D(32, 5,padding='same', kernel_initializer='he_uniform')(x)\n#cn = Dense(32, activation='tanh')(x)\n#x=Conv1D(32, 5,padding='same',activation='relu')(x)\nx= GlobalMaxPool1D()(x)\n#x = Bidirectional(LSTM(200, dropout=0.2, recurrent_dropout=0.2,return_sequences=True))()\n#x = Bidirectional(LSTM(32,dropout=0.1, recurrent_dropout=0.1))(x)#((jembedding_sequences))\n#x = Dense(32 ,activation='relu')(x)\n#x = Dense(32, kernel_regularizer=tf.keras.regularizers.l2(0.01),activation='relu')(x)\n#x = Conv1D(128, 3, activation='relu\n#y=Conv1D(200, 5,padding='same', kernel_initializer='he_uniform')(s)\n#y=Conv1D(150, 5,padding='same',activation='relu')(s)\n#y=Conv1D(100, 5,padding='same',activation='relu')(y)\n#y= GlobalMaxPool1D()(y)\n#y1=Conv1D(200, 5, padding='same', kernel_initializer='he_uniform', activation='relu')(s)\n#y1= GlobalMaxPool1D()(y1)\n#y2=Conv1D(64, 5, padding='same', kernel_initializer='he_uniform', activation='relu')(s)\n#x = LSTM(32, dropout=0.2, recurrent_dropout=0.2)(x)\n#y2= GlobalMaxPool1D()(y2)\n#z = Concatenate()([y, x])\n#z = Dense(256,activation='relu')(z)\n#z = Dense(256,kernel_regularizer=regularizers.l2(0.01),activation='relu')(z)\n#z = Dense(64, kernel_regularizer=regularizers.l2(0.01),activation='relu')(z)\n#x = Dense(10, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\nx = Dropout(0.5)(x)\n#x = Dense(10, activation='relu')(x)\noutputs = Dense(3, kernel_regularizer=tf.keras.regularizers.l2(0.01),activation='softmax')(x)\n#outputs = Dense(3,activation='softmax')(x)\nmodelse= tf.keras.Model(jsequence_input, outputs)\n#Model Compile\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nmodelse.compile(optimizer=Adam(learning_rate=0.0005), loss='categorical_crossentropy',\n              metrics=['categorical_accuracy'])\n\nhistoryse = modelse.fit(jx_train, y_train, epochs=50,batch_size=32, validation_split=0.1,validation_data=(jx_test, y_test))\n\ns, (at, al) = plt.subplots(2,1)\nat.plot(historyse.history['categorical_accuracy'], c= 'b')\nat.plot(historyse.history['val_categorical_accuracy'], c='r')\nat.set_title('model accuracy')\nat.set_ylabel('accuracy')\nat.set_xlabel('epoch')\nat.legend(['CNN_train', 'CNN_val'], loc='upper left')\n\nal.plot(historyse.history['loss'], c='m')\nal.plot(historyse.history['val_loss'], c='c')\nal.set_title('model loss')\nal.set_ylabel('loss')\nal.set_xlabel('epoch')\nal.legend(['train', 'val'], loc = 'upper left')\n#<matplotlib.legend.Legend at 0x7f3739c12490>\n#ypred=modelse.predict(x_test)\n#ypred=np.around(ypred)\npredicted = modelse.predict(jx_test)\nbscore=modelse.evaluate(jx_test,y_test)\nprint('Test Loss:', bscore[0])\nprint('Test accuracy:', bscore[1])\nypredicted = np.argmax(predicted,axis=1)\n#y_predicted = np.where(predicted>0.5,1,0)\nfrom sklearn.metrics import classification_report\nprint(classification_report(test_df.jlabel,ypredicted))\n#from sklearn.metrics import accuracy_score,jaccard_score,precision_score,recall_score,f1_score\n#print(\"Accuracy = \",(accuracy_score(y_test,ypred, normalize=True, sample_weight=None)*100))\n#print('Jaccard Accuracy %:',(jaccard_score(y_test,ypred, average='macro'))*100)\n#print('Precision_Score  %:',(precision_score(y_test, ypred, average ='macro'))*100)\n#print('F1_Score         %:',(f1_score(y_test,ypred, average='macro'))*100)\n#print('Recall_Score     %:',(recall_score(y_test, ypred, average='macro'))*100)\n","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Loading the data ","metadata":{"papermill":{"duration":0.083657,"end_time":"2022-09-18T13:23:17.630335","exception":false,"start_time":"2022-09-18T13:23:17.546678","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"here sample_df shows up the format of data to be uploaded on kaggle comptetions","metadata":{"papermill":{"duration":0.078752,"end_time":"2022-09-18T13:23:18.115701","exception":false,"start_time":"2022-09-18T13:23:18.036949","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"","metadata":{"papermill":{"duration":0.077831,"end_time":"2022-09-18T13:23:18.272117","exception":false,"start_time":"2022-09-18T13:23:18.194286","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### We need to do some text cleaning\nhere some signs and characters need to be removed , again cleaning the text data before training is a good practice well bert is more advanced architecture , it doesn't much affect if we dont do data cleaning \n, bert dont need extensive text_cleaning because bert comes with 40/60000 words hence its really not necessary to do text_cleaning but removing the special characters are good practice","metadata":{"papermill":{"duration":0.076649,"end_time":"2022-09-18T13:23:18.426564","exception":false,"start_time":"2022-09-18T13:23:18.349915","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#train_cleaned_df[train_cleaned_df.target == 0]\n#train_df = train_cleaned_df.copy()","metadata":{"papermill":{"duration":0.084744,"end_time":"2022-09-18T13:23:18.758019","exception":false,"start_time":"2022-09-18T13:23:18.673275","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.086516,"end_time":"2022-09-18T13:23:18.922863","exception":false,"start_time":"2022-09-18T13:23:18.836347","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"here target 1 means we are talking about any accident or disaster and 0 means just a formal tweets with not much attention\n\nso far we have cleaned our text data and now lets load our model","metadata":{"papermill":{"duration":0.079705,"end_time":"2022-09-18T13:23:19.080382","exception":false,"start_time":"2022-09-18T13:23:19.000677","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!pip install transformers","metadata":{"papermill":{"duration":7.281221,"end_time":"2022-09-18T13:23:26.440553","exception":false,"start_time":"2022-09-18T13:23:19.159332","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T11:43:55.132769Z","iopub.execute_input":"2024-04-08T11:43:55.133231Z","iopub.status.idle":"2024-04-08T11:44:06.564464Z","shell.execute_reply.started":"2024-04-08T11:43:55.133192Z","shell.execute_reply":"2024-04-08T11:44:06.563216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Loading Pretrained BERT Model","metadata":{"papermill":{"duration":0.07983,"end_time":"2022-09-18T13:23:26.601619","exception":false,"start_time":"2022-09-18T13:23:26.521789","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from transformers import AutoTokenizer,TFBertModel\nbtokenizer = AutoTokenizer.from_pretrained('bert-large-uncased')\nbert = TFBertModel.from_pretrained('bert-large-uncased')","metadata":{"papermill":{"duration":68.632652,"end_time":"2022-09-18T13:24:35.313408","exception":false,"start_time":"2022-09-18T13:23:26.680756","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T11:49:41.203422Z","iopub.execute_input":"2024-04-08T11:49:41.204532Z","iopub.status.idle":"2024-04-08T11:49:53.857793Z","shell.execute_reply.started":"2024-04-08T11:49:41.204488Z","shell.execute_reply":"2024-04-08T11:49:53.856812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tokenizer('this is me abhishek and i am a very bad boy &*&*&&')","metadata":{"papermill":{"duration":0.093184,"end_time":"2022-09-18T13:24:35.497378","exception":false,"start_time":"2022-09-18T13:24:35.404194","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Convert Our text data into BERT input format ","metadata":{"papermill":{"duration":0.089745,"end_time":"2022-09-18T13:24:35.668326","exception":false,"start_time":"2022-09-18T13:24:35.578581","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#print(\"max len of tweets\",max([len(x.split()) for x in train_df.cbts1]))\nprint(\"max len of tweets\",max([len(x.split()) for x in train_df.class_att_res50]))\n#print(\"max len of tweets\",max([len(x.split()) for x in train_df.object_attr1_text]))\n#max_length = 40class_att_res50","metadata":{"papermill":{"duration":0.098011,"end_time":"2022-09-18T13:24:36.040278","exception":false,"start_time":"2022-09-18T13:24:35.942267","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T13:22:07.808180Z","iopub.execute_input":"2024-04-08T13:22:07.809317Z","iopub.status.idle":"2024-04-08T13:22:07.829537Z","shell.execute_reply.started":"2024-04-08T13:22:07.809273Z","shell.execute_reply":"2024-04-08T13:22:07.828321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('helo')","metadata":{"execution":{"iopub.status.busy":"2024-04-08T11:46:15.301228Z","iopub.execute_input":"2024-04-08T11:46:15.302297Z","iopub.status.idle":"2024-04-08T11:46:15.314460Z","shell.execute_reply.started":"2024-04-08T11:46:15.302194Z","shell.execute_reply":"2024-04-08T11:46:15.313475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data processing for object class name**","metadata":{"papermill":{"duration":0.086469,"end_time":"2022-09-18T13:24:36.213752","exception":false,"start_time":"2022-09-18T13:24:36.127283","status":"completed"},"tags":[]}},{"cell_type":"code","source":"''''cx_train = tokenizer(\n    text=train_df.object_list.tolist(),\n    add_special_tokens=True,\n    max_length=40,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)\ncx_test = tokenizer(\n    text=test_df.object_list.tolist(),\n    add_special_tokens=True,\n    max_length=40,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)\ny_train=pd.get_dummies(train_df.jlabel)\ny_test=pd.get_dummies(test_df.jlabel)\nprint(cx_train['input_ids'].shape,cx_train['attention_mask'].shape,cx_test['input_ids'].shape,y_train.shape,y_train.shape)'''","metadata":{"papermill":{"duration":0.08993,"end_time":"2022-09-18T13:24:36.385486","exception":false,"start_time":"2022-09-18T13:24:36.295556","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train and test Data process for clean text**","metadata":{"papermill":{"duration":0.142756,"end_time":"2022-09-18T13:24:36.610190","exception":false,"start_time":"2022-09-18T13:24:36.467434","status":"completed"},"tags":[]}},{"cell_type":"code","source":"tx_train = btokenizer(\n    text=train_df.class_att_res50.tolist(),\n    #text=train_df.object_attr1_text.tolist(),\n    add_special_tokens=True,\n    max_length=60,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)\n'''tx_val = btokenizer(\n    text=val_df.object_attr_text.tolist(),\n    add_special_tokens=True,\n    max_length=25,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)'''\n\ntx_test = btokenizer(\n    text=test_df.class_att_res50.tolist(),\n    #text=test_df.object_attr1_text.tolist(),\n    add_special_tokens=True,\n    max_length=60,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)\nprint(tx_train['input_ids'].shape,tx_train['attention_mask'].shape,tx_test['input_ids'].shape)\n#print(tx_val['input_ids'].shape,tx_val['attention_mask'].shape)\n#jy_train=pd.get_dummies(train_df.joint_label)\n#jy_test=pd.get_dummies(test_df.joint_label)\n#print(jy_train.shape,jy_test.shape)","metadata":{"papermill":{"duration":0.194861,"end_time":"2022-09-18T13:24:36.929573","exception":false,"start_time":"2022-09-18T13:24:36.734712","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T15:13:07.989126Z","iopub.execute_input":"2024-04-08T15:13:07.989638Z","iopub.status.idle":"2024-04-08T15:13:08.392512Z","shell.execute_reply.started":"2024-04-08T15:13:07.989601Z","shell.execute_reply":"2024-04-08T15:13:08.391520Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Building the model architecture ","metadata":{"papermill":{"duration":0.133322,"end_time":"2022-09-18T13:24:37.208659","exception":false,"start_time":"2022-09-18T13:24:37.075337","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len = 60\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense\n\ninput_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\ninput_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n# embeddings = dbert_model(input_ids,attention_mask = input_mask)[0]\n\n\nembeddings = bert(input_ids,attention_mask = input_mask)[1] #(0 is the last hidden states,1 means pooler_output)\n# out = tf.keras.layers.GlobalMaxPool1D()(embeddings)\nout = Dropout(0.1)(embeddings)\n#bert_text = Dense(256,activation = 'tanh')(embeddings)#(out)\n#bert_text = Dense(1024,activation = 'relu')(embeddings)#(out)\nout = Dense(32, activation='relu')(out)\n#bout=Flatten()(embeddings)\n#out = Dense(128, activation='relu')(out)\n#out = Dropout(0.1)(out)\n#out = Dense(32, activation='relu')(out)\nout = Dropout(0.1)(out)\n#y = Dense(1,activation = 'sigmoid')(out)\ny = Dense(3,activation = 'softmax')(out)\n    \nmodel = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\nmodel.layers[2].trainable = True#False#\n# for training bert our lr must be so small'''\n","metadata":{"papermill":{"duration":0.100163,"end_time":"2022-09-18T13:24:37.984197","exception":false,"start_time":"2022-09-18T13:24:37.884034","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T15:13:15.746206Z","iopub.execute_input":"2024-04-08T15:13:15.746591Z","iopub.status.idle":"2024-04-08T15:13:18.801277Z","shell.execute_reply.started":"2024-04-08T15:13:15.746559Z","shell.execute_reply":"2024-04-08T15:13:18.800423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom tensorflow.keras.losses import CategoricalCrossentropy,BinaryCrossentropy\nfrom tensorflow.keras.metrics import CategoricalAccuracy,BinaryAccuracy\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.layers import Input,Dense, Dropout, Flatten,BatchNormalization,concatenate,Multiply\nfrom tensorflow.python.keras import regularizers","metadata":{"papermill":{"duration":0.138379,"end_time":"2022-09-18T13:24:37.473620","exception":false,"start_time":"2022-09-18T13:24:37.335241","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T15:13:26.442836Z","iopub.execute_input":"2024-04-08T15:13:26.443285Z","iopub.status.idle":"2024-04-08T15:13:26.451007Z","shell.execute_reply.started":"2024-04-08T15:13:26.443248Z","shell.execute_reply":"2024-04-08T15:13:26.449872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''# Bert + LSTM text model\ninput_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\ninput_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\ninput_segments = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n#input_ids = layers.Input(shape=(max_length,), dtype=tf.int32, name=\"input_word_ids\")\n#input_masks = layers.Input(shape=(max_length,), dtype=tf.int32, name=\"input_masks\")\n#input_segments = layers.Input(shape=(max_length,), dtype=tf.int32, name=\"segment_ids\")\n#embeddings = bert(input_ids,attention_mask = input_mask)[1]\n#_, seq_out = bert([input_ids, input_mask,input_segments])[1]\n#sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nseq_out = bert([input_ids, input_mask,input_segments])[1]\nout = tf.keras.layers.LSTM(128, name='LSTM')(seq_out)#(embeddings)\nbert_lstm = models.Model([input_ids, input_mask], out)'''","metadata":{"papermill":{"duration":0.09648,"end_time":"2022-09-18T13:24:38.169936","exception":false,"start_time":"2022-09-18T13:24:38.073456","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''''max_len = 25\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense\n\ninput_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\ninput_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n# embeddings = dbert_model(input_ids,attention_mask = input_mask)[0]\n\n\nembeddings = bert(input_ids,attention_mask = input_mask)[0] #(0 is the last hidden states,1 means pooler_output)\n#out = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n#tbert_text = Dense(512, activation='relu')(out)\n#out = tf.keras.layers.Dropout(0.1)(tbert_text)#(out)\n#out = Dense(512, activation='relu')(out)\n#out = tf.keras.layers.Dropout(0.5)(out)\n#out = Dense(128, activation='relu')(out)\nbout = tf.keras.layers.Flatten()(embeddings)#(out)\n#out = Dense(32,activation = 'relu')(out)\n#out = tf.keras.layers.Dropout(0.1)(out)\ny = Dense(3,activation = 'sigmoid')(bout)\n    \ntbmodel = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\ntbmodel.layers[2].trainable = True\n# for training bert our lr must be so small'''","metadata":{"papermill":{"duration":7.565471,"end_time":"2022-09-18T13:24:45.996265","exception":false,"start_time":"2022-09-18T13:24:38.430794","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"papermill":{"duration":0.083863,"end_time":"2022-09-18T13:24:46.370037","exception":false,"start_time":"2022-09-18T13:24:46.286174","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"","metadata":{"papermill":{"duration":0.083532,"end_time":"2022-09-18T13:24:46.540167","exception":false,"start_time":"2022-09-18T13:24:46.456635","status":"completed"},"tags":[]}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = Adam(\n    learning_rate=3e-06,#6e-06 # this learning rate is for bert model , taken from huggingface website \n    epsilon=1e-08,#1e-08\n    decay=1e-5,#,#1e-5\n    clipnorm=1.0)\n\n# Set loss and metrics\nloss = CategoricalCrossentropy(from_logits = True)\n#metric = CategoricalAccuracy('balanced_accuracy'),\n\nmetric = [\n       tf.keras.metrics.CategoricalAccuracy(name='accuracy'),\n       tf.keras.metrics.Precision(name='precision'),\n       tf.keras.metrics.Recall(name='recall')\n]\n# Compile the model\nmodel.compile(\n    optimizer = optimizer,\n    loss = loss, \n    metrics = metric)","metadata":{"papermill":{"duration":0.137581,"end_time":"2022-09-18T13:24:46.761480","exception":false,"start_time":"2022-09-18T13:24:46.623899","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T15:13:32.872258Z","iopub.execute_input":"2024-04-08T15:13:32.872937Z","iopub.status.idle":"2024-04-08T15:13:32.903103Z","shell.execute_reply.started":"2024-04-08T15:13:32.872901Z","shell.execute_reply":"2024-04-08T15:13:32.902354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot_model(bmodel, show_shapes = True)","metadata":{"papermill":{"duration":0.091166,"end_time":"2022-09-18T13:24:46.939393","exception":false,"start_time":"2022-09-18T13:24:46.848227","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ntf.config.experimental.list_physical_devices('GPU')","metadata":{"papermill":{"duration":0.093107,"end_time":"2022-09-18T13:24:47.118040","exception":false,"start_time":"2022-09-18T13:24:47.024933","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T11:56:20.123153Z","iopub.execute_input":"2024-04-08T11:56:20.123558Z","iopub.status.idle":"2024-04-08T11:56:20.130569Z","shell.execute_reply.started":"2024-04-08T11:56:20.123523Z","shell.execute_reply":"2024-04-08T11:56:20.129570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### TESTING PHASE\non this phase we will make predictions out of our model and then submit to kaggle comptetions","metadata":{"papermill":{"duration":0.084069,"end_time":"2022-09-18T13:24:47.287336","exception":false,"start_time":"2022-09-18T13:24:47.203267","status":"completed"},"tags":[]}},{"cell_type":"code","source":"''''def get_label_code(text):\n    if text=='positive':\n        return 2\n    elif text=='neutral':\n        return 1\n    else :\n        return 0\ntest_df['jlabel']=test_df['jsentiment'].apply(get_label_code)\ntest_df['vlabel']=test_df['vsentiment'].apply(get_label_code)\n#test_df[:2]'''","metadata":{"papermill":{"duration":0.092899,"end_time":"2022-09-18T13:24:47.465691","exception":false,"start_time":"2022-09-18T13:24:47.372792","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Bert model for text sentment for clean text**","metadata":{"papermill":{"duration":0.110099,"end_time":"2022-09-18T13:24:47.661019","exception":false,"start_time":"2022-09-18T13:24:47.550920","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"","metadata":{"papermill":{"duration":0.084757,"end_time":"2022-09-18T13:24:47.846027","exception":false,"start_time":"2022-09-18T13:24:47.761270","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#train_df.shape\ntest_df.joint_sentiment.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Max length 80 textual sentiment  with  objet+ctext with tlabel\n\ntrain_history = model.fit(\n    x ={'input_ids':tx_train['input_ids'],'attention_mask':tx_train['attention_mask']} ,\n    y = y_train,#to_categorical(train_df.tlabel),\n   #validation_data = (\n   # {'input_ids':tx_val['input_ids'],'attention_mask':tx_val['attention_mask']},y_val #to_categorical(test_df.tlabel)\n   #),\n    validation_split = 0.1,\n  epochs=30,\n    batch_size=32\n)\n\n#text classification only object_class name  with visual text\npredicted = model.predict({'input_ids':tx_test['input_ids'],'attention_mask':tx_test['attention_mask']})\nbscore=model.evaluate({'input_ids':tx_test['input_ids'],'attention_mask':tx_test['attention_mask']},y_test)\nprint('Test Loss:', bscore[0])\nprint('Test accuracy:', bscore[1])\nypredicted = np.argmax(predicted,axis=1)\n#y_predicted = np.where(predicted>0.5,1,0)\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\nprint(classification_report(test_df.ilabel,ypredicted,digits=4))\n#accuracy vs loss function\ns, (at, al) = plt.subplots(2,1)\nat.plot(train_history.history['accuracy'], c= 'b')\nat.plot(train_history.history['val_accuracy'], c='r')\nat.set_title('model accuracy')\nat.set_ylabel('accuracy')\nat.set_xlabel('epoch')\nat.legend(['BERT_train', 'BERT_val'], loc='upper left')\n\nal.plot(train_history.history['loss'], c='m')\nal.plot(train_history.history['val_loss'], c='c')\nal.set_title('model loss')\nal.set_ylabel('loss')\nal.set_xlabel('epoch')\nal.legend(['train', 'val'], loc = 'upper left')\nfrom sklearn.metrics import balanced_accuracy_score,recall_score,precision_score,f1_score\nbal_acc=balanced_accuracy_score(test_df.ilabel,ypredicted)\nrecall=recall_score(test_df.ilabel,ypredicted,average='weighted')\nprecision=precision_score(test_df.ilabel,ypredicted,average='weighted')\nf1=f1_score(test_df.ilabel,ypredicted,average='weighted')\nprint(bal_acc,recall,precision,f1)","metadata":{"papermill":{"duration":121.596867,"end_time":"2022-09-18T13:26:49.897500","exception":false,"start_time":"2022-09-18T13:24:48.300633","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T16:10:42.953079Z","iopub.execute_input":"2024-04-08T16:10:42.954227Z","iopub.status.idle":"2024-04-08T16:36:56.379018Z","shell.execute_reply.started":"2024-04-08T16:10:42.954184Z","shell.execute_reply":"2024-04-08T16:36:56.377706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted = model.predict({'input_ids':tx_test['input_ids'],'attention_mask':tx_test['attention_mask']})\nbscore=model.evaluate({'input_ids':tx_test['input_ids'],'attention_mask':tx_test['attention_mask']},y_test)\nprint('Test Loss:', bscore[0])\nprint('Test accuracy:', bscore[1])\nypredicted = np.argmax(predicted,axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T16:57:49.550641Z","iopub.execute_input":"2024-04-08T16:57:49.551077Z","iopub.status.idle":"2024-04-08T16:57:54.546580Z","shell.execute_reply.started":"2024-04-08T16:57:49.551040Z","shell.execute_reply":"2024-04-08T16:57:54.545178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\nprint(classification_report(test_df.label,ypredicted,digits=4))\n#accuracy vs loss function\ns, (at, al) = plt.subplots(2,1)\nat.plot(train_history.history['accuracy'], c= 'b')\nat.plot(train_history.history['val_accuracy'], c='r')\nat.set_title('model accuracy')\nat.set_ylabel('accuracy')\nat.set_xlabel('epoch')\nat.legend(['BERT_train', 'BERT_val'], loc='upper left')\n\nal.plot(train_history.history['loss'], c='m')\nal.plot(train_history.history['val_loss'], c='c')\nal.set_title('model loss')\nal.set_ylabel('loss')\nal.set_xlabel('epoch')\nal.legend(['train', 'val'], loc = 'upper left')\nfrom sklearn.metrics import balanced_accuracy_score,recall_score,precision_score,f1_score\nbal_acc=balanced_accuracy_score(test_df.label,ypredicted)\nrecall=recall_score(test_df.label,ypredicted,average='weighted')\nprecision=precision_score(test_df.label,ypredicted,average='weighted')\nf1=f1_score(test_df.label,ypredicted,average='weighted')\nprint(bal_acc,recall,precision,f1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import balanced_accuracy_score\nbal_acc=balanced_accuracy_score(test_df.jlabel,ypredicted)\nrecall=recall_score(test_df.jlabel,ypredicted,average='weighted')\nprecision=precision_score(test_df.jlabel,ypredicted,average='weighted')\nf1=f1_score(test_df.jlabel,ypredicted,average='weighted')\nprint(bal_acc,recall,precision,f1)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-26T13:59:29.412424Z","iopub.execute_input":"2024-02-26T13:59:29.413444Z","iopub.status.idle":"2024-02-26T13:59:29.426754Z","shell.execute_reply.started":"2024-02-26T13:59:29.413399Z","shell.execute_reply":"2024-02-26T13:59:29.425773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#balance data for joint sentiment object_attribute_scene+text\n\nfrom sklearn.metrics import balanced_accuracy_score\nbal_acc=balanced_accuracy_score(test_df.jlabel,ypredicted)\nrecall=recall_score(test_df.jlabel,ypredicted,average='weighted')\nprecision=precision_score(test_df.jlabel,ypredicted,average='weighted')\nf1=f1_score(test_df.jlabel,ypredicted,average='weighted')\nprint(bal_acc,recall,precision,f1)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-26T13:29:18.474729Z","iopub.execute_input":"2024-02-26T13:29:18.475310Z","iopub.status.idle":"2024-02-26T13:29:18.488694Z","shell.execute_reply.started":"2024-02-26T13:29:18.475263Z","shell.execute_reply":"2024-02-26T13:29:18.487588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n\n# Compute confusion matrix\ncm = confusion_matrix(test_df.jlabel, ypredicted)\n\n# Plot confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g', xticklabels=[0, 1, 2], yticklabels=[0, 1, 2])\nplt.xlabel('Predicted labels')\nplt.ylabel('True labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-26T14:02:00.733550Z","iopub.execute_input":"2024-02-26T14:02:00.734679Z","iopub.status.idle":"2024-02-26T14:02:01.095896Z","shell.execute_reply.started":"2024-02-26T14:02:00.734632Z","shell.execute_reply":"2024-02-26T14:02:01.094702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import scipy.stats as stats\n\n# Predicted sentiment scores\n#predicted_scores = [3.2, 4.1, 2.9, 3.7, 4.5]\n# Ground truth sentiment scores\n#ground_truth_scores = [3.5, 3.9, 2.8, 4.0, 4.2]\n\n# Perform paired t-test\nt_statistic, p_value = stats.ttest_rel(ypredicted, test_df.jlabel)\n\n# Check significance level (alpha)\nalpha = 0.05\nif p_value < alpha:\n    print(\"There is a significant difference between predicted and ground truth sentiment scores.\")\nelse:\n    print(\"There is no significant difference between predicted and ground truth sentiment scores.\")","metadata":{"execution":{"iopub.status.busy":"2024-02-26T14:03:34.413907Z","iopub.execute_input":"2024-02-26T14:03:34.414947Z","iopub.status.idle":"2024-02-26T14:03:34.424342Z","shell.execute_reply.started":"2024-02-26T14:03:34.414904Z","shell.execute_reply":"2024-02-26T14:03:34.423312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pdf = pd.DataFrame(data={'y_test': test_df.jlabel, 'y_pred': ypredicted})\n\npdf.to_csv('./simpson_class_attr_scene_pred_bert.csv', index=False)\n#run['test/predictions'] = neptune.types.File.as_html(df)\n#pdf[:2]","metadata":{"execution":{"iopub.status.busy":"2024-02-26T14:04:09.933409Z","iopub.execute_input":"2024-02-26T14:04:09.934163Z","iopub.status.idle":"2024-02-26T14:04:09.941808Z","shell.execute_reply.started":"2024-02-26T14:04:09.934122Z","shell.execute_reply":"2024-02-26T14:04:09.940789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# help\n''''_input = Input(shape=[max_length], dtype='int32')\n\n# get the embedding layer\nembedded = Embedding(\n        input_dim=vocab_size,\n        output_dim=embedding_size,\n        input_length=max_length,\n        trainable=False,\n        mask_zero=False\n    )(_input)\n\nactivations = LSTM(units, return_sequences=True)(embedded)\n\n# compute importance for each step\nattention = Dense(1, activation='tanh')(activations)\nattention = Flatten()(attention)\nattention = Activation('softmax')(attention)\nattention = RepeatVector(units)(attention)\nattention = Permute([2, 1])(attention)\n\n\nsent_representation = merge([activations, attention], mode='mul')\nsent_representation = Lambda(lambda xin: K.sum(xin, axis=-2), output_shape=(units,))(sent_representation)\n\nprobabilities = Dense(3, activation='softmax')(sent_representation)\n\n\n\nAdvices\nactivations = LSTM(units, return_sequences=True)(embedded)\n\n# compute importance for each step\nattention = Dense(1, activation='tanh')(activations)\nattention = Flatten()(attention)\nattention = Activation('softmax')(attention)\nattention = RepeatVector(units)(attention)\nattention = Permute([2, 1])(attention)\n\nsent_representation = merge([activations, attention], mode='mul')\n\nattention = Activation('softmax')(attention)\n\n\n#another one\ndef build_model():\n  input_dims = train_data_X.shape[1]\n  inputs = Input(shape=(input_dims,))\n  dense1800 = Dense(1800, activation='relu', kernel_regularizer=regularizers.l2(0.01))(inputs)\n  attention_probs = Dense( 1800, activation='sigmoid', name='attention_probs')(dense1800)\n  attention_mul = multiply([ dense1800, attention_probs], name='attention_mul')\n  dense7 = Dense(7, kernel_regularizer=regularizers.l2(0.01), activation='softmax')(attention_mul)   \n  model = Model(input=[inputs], output=dense7)\n  model.compile(optimizer='adam',\n                loss='categorical_crossentropy',\n                metrics=['accuracy'])\n  return model\n\nprint (model.summary)\n\nmodel.fit( train_data_X, train_data_Y_, epochs=20, validation_split=0.2, batch_size=600, shuffle=True, verbose=1)\n'''","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**VISUAL Features extraction**","metadata":{"papermill":{"duration":0.193996,"end_time":"2022-09-18T13:26:50.632249","exception":false,"start_time":"2022-09-18T13:26:50.438253","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import cv2\n\n#collect all images from original file of Simpson\nimpath_list=train_df['img_path'].tolist()\n#sentiment=imdf['visual_label'].tolist()\npath='/kaggle/input/simpson-original-data/Dataset/Dataset/'\n#path='../input/mvsasingle/MVSA_Single/data/'\n\ntrain_img_file=[]\n#sent=[]\nimg_size=(224,224)\n#for file,s in zip(impath_list,sentiment):\nfor file in impath_list:\n    #nfile=str(file)\n    #img=cv2.imread(path+nfile+'.jpg')\n    img=cv2.imread(path+file)\n    img=cv2.resize(img,img_size)\n    train_img_file.append(img)\n    #sent.append(s)\n#convert images into array\n#imgarx_train=np.array(train_img_file)\nvx_train=np.array(train_img_file)\nprint(vx_train.shape)\n#dummies for labels\n#print(len(sent))\n#dff=pd.DataFrame()\n#dff['sentiment']=pd.DataFrame(sent)\n","metadata":{"papermill":{"duration":0.170771,"end_time":"2022-09-18T13:26:52.258079","exception":false,"start_time":"2022-09-18T13:26:52.087308","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-20T18:50:38.329844Z","iopub.execute_input":"2024-02-20T18:50:38.330271Z","iopub.status.idle":"2024-02-20T18:51:26.689755Z","shell.execute_reply.started":"2024-02-20T18:50:38.330235Z","shell.execute_reply":"2024-02-20T18:51:26.688640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import cv2\n#collect all images from original file of Simpson\nimpath_list=test_df['img_path'].tolist()\n#sentiment=imdf['visual_label'].tolist()\npath='/kaggle/input/simpson-original-data/Dataset/Dataset/'\n#path='../input/mvsasingle/MVSA_Single/data/'\ntest_img_file=[]\n#sent=[]\nimg_size=(224,224)\n#for file,s in zip(impath_list,sentiment):\nfor file in impath_list:\n    #nfile=str(file)\n    #img=cv2.imread(path+nfile+'.jpg')\n    img=cv2.imread(path+file)\n    img=cv2.resize(img,img_size)\n    test_img_file.append(img)\n    #sent.append(s)\n#convert images into array\nvx_test=np.array(test_img_file)\nprint(vx_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T18:52:03.884773Z","iopub.execute_input":"2024-02-20T18:52:03.885946Z","iopub.status.idle":"2024-02-20T18:52:16.363332Z","shell.execute_reply.started":"2024-02-20T18:52:03.885889Z","shell.execute_reply":"2024-02-20T18:52:16.362221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications import VGG16,VGG19,ResNet50\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input,Dense, Dropout, Flatten,BatchNormalization,concatenate,Bidirectional,LSTM\nfrom keras.layers import BatchNormalization\nimport matplotlib.pyplot as plt","metadata":{"papermill":{"duration":0.21372,"end_time":"2022-09-18T13:26:52.642503","exception":false,"start_time":"2022-09-18T13:26:52.428783","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-20T18:52:42.352416Z","iopub.execute_input":"2024-02-20T18:52:42.352827Z","iopub.status.idle":"2024-02-20T18:52:42.360428Z","shell.execute_reply.started":"2024-02-20T18:52:42.352792Z","shell.execute_reply":"2024-02-20T18:52:42.359557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate convolutional base mode for whole image\n\nimage_input = Input(shape=(224, 224, 3), name='image16')\n#base = VGG16(weights='imagenet',include_top=False,input_shape=(224, 224, 3))#(image_input)\nbase = VGG19(weights='imagenet',include_top=False,input_shape=(224, 224, 3))#(image_input)\n#base = ResNet50(weights='imagenet',include_top=False,input_shape=(224, 224, 3))#(image_input)\nfor layer in base.layers:\n    layer.trainable=False\n\nout = Flatten()(base.output) #(vgg16)#\nout = Dense(256,activation = 'relu')(out)#(out)\n#img_layer=Dense(4024, activation='relu')(vgg16.output)\nimg_model=Dropout(0.5)(out)\nvprediction = Dense(3, activation='softmax')(img_model)\nwmodel = tf.keras.Model(inputs=base.input, outputs=vprediction)\n#wmodel.summary()\n#img_model.summary()\nwmodel.compile(loss='categorical_crossentropy',\n                    optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),\n                    metrics=['accuracy'])#metric)#['accuracy'])\n\nwith tf.device('/device:GPU:0'):\n  com_history = wmodel.fit(\n      x =vx_train,\n      y = y_train,#to_categorical(train_df.tlabel),\n      #validation_data = ([vx_test,\n      #{'input_ids':tx_test['input_ids'],'attention_mask':tx_test['attention_mask']}],jy_test #to_categorical(test_df.tlabel)\n      #),\n    validation_split = 0.1,\n    epochs=10,\n      batch_size=32\n  )\n\n#text classification without english stop word and max word length 80\ncpredicted = wmodel.predict(vx_test)\n#com_score=wmodel.evaluate(vx_test,y_test)\n#print('Test Loss:', com_score[0])\n#print('Test accuracy:', com_score[1])\ncypredicted = np.argmax(cpredicted,axis=1)\n#y_predicted = np.where(predicted>0.5,1,0)\nfrom sklearn.metrics import classification_report\nprint(classification_report(test_df.ilabel,cypredicted,digits=4))\n#accuracy vs loss function\ns, (at, al) = plt.subplots(2,1)\nat.plot(com_history.history['accuracy'], c= 'b')\nat.plot(com_history.history['val_accuracy'], c='r')\nat.set_title('model accuracy')\nat.set_ylabel('accuracy')\nat.set_xlabel('epoch')\nat.legend(['BERT_train', 'BERT_val'], loc='upper left')\n\nal.plot(com_history.history['loss'], c='m')\nal.plot(com_history.history['val_loss'], c='c')\nal.set_title('model loss')\nal.set_ylabel('loss')\nal.set_xlabel('epoch')\nal.legend(['train', 'val'], loc = 'upper left')\n","metadata":{"execution":{"iopub.status.busy":"2024-02-20T18:55:59.547147Z","iopub.execute_input":"2024-02-20T18:55:59.548061Z","iopub.status.idle":"2024-02-20T18:56:54.368049Z","shell.execute_reply.started":"2024-02-20T18:55:59.548021Z","shell.execute_reply":"2024-02-20T18:56:54.367164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import balanced_accuracy_score\nfrom sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\n#print(\"Accuracy = \",(accuracy_score(y_test,ypred, normalize=True, sample_weight=None)*100))\n#print('Jaccard Accuracy %:',(balanced_accuracy_score(y_test,ypred, average='macro'))*100)\n#print('Precision_Score  %:',(precision_score(y_test, ypred, average ='macro'))*100)\n#print('F1_Score         %:',(f1_score(y_test,ypred, average='macro'))*100)\n#print('Recall_Score     %:',(recall_score(y_test, ypred, average='macro'))*100)\nprint(classification_report(test_df.ilabel,cypredicted,digits=4))\nacc=accuracy_score(test_df.ilabel,cypredicted)\nbal_acc=balanced_accuracy_score(test_df.ilabel,cypredicted)\nrecall=recall_score(test_df.ilabel,cypredicted,average='weighted')\nprecision=precision_score(test_df.ilabel,cypredicted,average='weighted')\nf1=f1_score(test_df.ilabel,cypredicted,average='weighted')\nprint(acc,bal_acc,recall,precision,f1)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T18:57:02.322921Z","iopub.execute_input":"2024-02-20T18:57:02.323714Z","iopub.status.idle":"2024-02-20T18:57:02.343066Z","shell.execute_reply.started":"2024-02-20T18:57:02.323666Z","shell.execute_reply":"2024-02-20T18:57:02.341955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate convolutional base mode for whole image\n\nimage_input = Input(shape=(224, 224, 3), name='image16')\nvgg16 = VGG16(weights='imagenet',include_top=False,input_shape=(224, 224, 3))#(image_input)\nfor layer in vgg16.layers:\n    layer.trainable=False\n\n#v16_layer = Flatten()(vgg16.output) #(vgg16)#\nv16_layer = Dense(256,activation = 'relu')(vgg16.output)#(out)\n#img_layer=Dense(4024, activation='relu')(vgg16.output)\nimg_model=Dropout(0.5)(v16_layer)\nvprediction = Dense(3, activation='softmax')(img_model)\nwmodel = tf.keras.Model(inputs=vgg16.input, outputs=vprediction)\n#wmodel.summary()\n#img_model.summary()\nwmodel.compile(loss='categorical_crossentropy',\n                    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n                    metrics=metric)#['accuracy'])\n","metadata":{"papermill":{"duration":0.311322,"end_time":"2022-09-18T13:45:16.521302","exception":false,"start_time":"2022-09-18T13:45:16.209980","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenating the last layer of two model global+bert\nv16_layer=Flatten()(v16_layer)\nbert_text=Flatten()(bert_text)\ncon_layer = concatenate([v16_layer,bert_text])\ncon_layer=Flatten()(con_layer)\n#con_layer=LSTM(128, return_sequences=True)(con_layer)\n#x = LSTM(512, return_sequences=False)(x)\n#con_out = Dense(2048,activation='relu')(con_layer)\n#con_out = Dropout(0.5)(con_out)\n#con_out = Dense(1024,activation='sigmoid')(con_out)\ncon_out = Dropout(0.2)(con_layer)\ncon_out = Dense(3,activation='softmax')(con_out)\n#out = Dense(3,activation='softmax')(con_layer)\n\n# Defining model input and output\ncom_model = tf.keras.Model(inputs = [wmodel.input,model.input], outputs=con_out)\n#com_model.summary()\n# Using Stochastic gradient descent with optimizer\n# Compile the model\ncom_model.compile(\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4),#optimizer,#\n    loss = 'categorical_crossentropy',#loss\n    metrics = metric)#Max length 55 with ctext with class name with joint label\ncom_history = com_model.fit(\n    x =[vx_train,{'input_ids':tx_train['input_ids'],'attention_mask':tx_train['attention_mask']}],\n    y = y_train,#to_categorical(train_df.tlabel),\n    #validation_data = ([vx_test,\n    #{'input_ids':tx_test['input_ids'],'attention_mask':tx_test['attention_mask']}],jy_test #to_categorical(test_df.tlabel)\n    #),\n  validation_split = 0.1,\n  epochs=10,\n    batch_size=32\n)\n\n#text classification without english stop word and max word length 80\ncpredicted = com_model.predict([vx_test,{'input_ids':tx_test['input_ids'],'attention_mask':tx_test['attention_mask']}])\ncom_score=com_model.evaluate([vx_test,{'input_ids':tx_test['input_ids'],'attention_mask':tx_test['attention_mask']}],y_test)\nprint('Test Loss:', com_score[0])\nprint('Test accuracy:', com_score[1])\ncypredicted = np.argmax(cpredicted,axis=1)\n#y_predicted = np.where(predicted>0.5,1,0)\nfrom sklearn.metrics import classification_report\nprint(classification_report(test_df.jlabel,cypredicted))\n#accuracy vs loss function\ns, (at, al) = plt.subplots(2,1)\nat.plot(com_history.history['accuracy'], c= 'b')\nat.plot(com_history.history['val_accuracy'], c='r')\nat.set_title('model accuracy')\nat.set_ylabel('accuracy')\nat.set_xlabel('epoch')\nat.legend(['BERT_train', 'BERT_val'], loc='upper left')\n\nal.plot(com_history.history['loss'], c='m')\nal.plot(com_history.history['val_loss'], c='c')\nal.set_title('model loss')\nal.set_ylabel('loss')\nal.set_xlabel('epoch')\nal.legend(['train', 'val'], loc = 'upper left')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate convolutional base mode for whole image\nimage_input = Input(shape=(224, 224, 3), name='image')\nresnet = ResNet50(weights='imagenet',\n                  include_top=False,\n                  input_shape=(224, 224, 3))#(image_input)\nfor layer in resnet.layers:\n    layer.trainable=False\nres_layer = Dense(256, activation='relu')(resnet.output)\nres_layer = Flatten()(res_layer) #(vgg16)#\nres_layer_32=Dense(32, activation='relu')(res_layer)\nimg_model=Dropout(0.5)(res_layer_32)\nvprediction = Dense(3, activation='softmax')(img_model)\nrmodel = tf.keras.Model(inputs=resnet.input, outputs=vprediction)\n#rmodel.summary()\n#img_model.summary()\n#rmodel.compile(loss='categorical_crossentropy',\n#                    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n #                   metrics=metric)#['accuracy'])","metadata":{"papermill":{"duration":0.31506,"end_time":"2022-09-18T13:45:17.151687","exception":false,"start_time":"2022-09-18T13:45:16.836627","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-20T18:57:30.732915Z","iopub.execute_input":"2024-02-20T18:57:30.733626Z","iopub.status.idle":"2024-02-20T18:57:32.978222Z","shell.execute_reply.started":"2024-02-20T18:57:30.733586Z","shell.execute_reply":"2024-02-20T18:57:32.977097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenating the last layer of two model global+bert\nres_layer=Flatten()(res_layer)\nbert_text=Flatten()(bert_text)\n\n\ncon_layer = concatenate([res_layer,bert_text])\n#con_layer=Flatten()(con_layer)\n#con_layer=LSTM(128, return_sequences=True)(con_layer)\n#x = LSTM(512, return_sequences=False)(x)\ncon_out = Dense(256,activation='relu')(con_layer)\n#con_out = Dropout(0.5)(con_out)\n#con_out = Dense(1024,activation='sigmoid')(con_out)\ncon_out = Dropout(0.2)(con_out)\ncon_out = Dense(3, kernel_regularizer=tf.keras.regularizers.l2(0.01),activation='softmax')(con_out)\n#out = Dense(3,activation='softmax')(con_layer)\n\n# Defining model input and output\ncom_model = tf.keras.Model(inputs = [rmodel.input,model.input], outputs=con_out)\n#com_model.summary()\n# Using Stochastic gradient descent with optimizer\n# Compile the model\ncom_model.compile(\n    optimizer =tf.keras.optimizers.Adam(learning_rate=0.0001),# optimizer,#\n    loss = 'categorical_crossentropy',#loss\n    metrics = ['categorical_accuracy'])#metric)\n#Max length 55 with ctext with class name with joint label\ncom_history = com_model.fit(\n    x =[vx_train,{'input_ids':tx_train['input_ids'],'attention_mask':tx_train['attention_mask']}],\n    y = y_train,#to_categorical(train_df.tlabel),\n    #validation_data = ([vx_test,\n    #{'input_ids':tx_test['input_ids'],'attention_mask':tx_test['attention_mask']}],y_test #to_categorical(test_df.tlabel)\n   # ),\n validation_split = 0.1,\n  epochs=10,\n    batch_size=32\n)\n\n#text classification without english stop word and max word length 80\ncpredicted = com_model.predict([vx_test,{'input_ids':tx_test['input_ids'],'attention_mask':tx_test['attention_mask']}])\ncom_score=com_model.evaluate([vx_test,{'input_ids':tx_test['input_ids'],'attention_mask':tx_test['attention_mask']}],y_test)\nprint('Test Loss:', com_score[0])\nprint('Test accuracy:', com_score[1])\ncypredicted = np.argmax(cpredicted,axis=1)\n#y_predicted = np.where(predicted>0.5,1,0)\nfrom sklearn.metrics import classification_report\nprint(classification_report(test_df.jlabel,cypredicted))\n#accuracy vs loss function\ns, (at, al) = plt.subplots(2,1)\nat.plot(com_history.history['accuracy'], c= 'b')\nat.plot(com_history.history['val_accuracy'], c='r')\nat.set_title('model accuracy')\nat.set_ylabel('accuracy')\nat.set_xlabel('epoch')\nat.legend(['Res_BERT_train', 'Res_BERT_val'], loc='upper left')\n\nal.plot(com_history.history['loss'], c='m')\nal.plot(com_history.history['val_loss'], c='c')\nal.set_title('model loss')\nal.set_ylabel('loss')\nal.set_xlabel('epoch')\nal.legend(['train', 'val'], loc = 'upper left')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenating the last layer of two model global+bert\nres_layer=Flatten()(res_layer)\nbilstm=Flatten()(bilstm)\ncon_layer = concatenate([res_layer,bilstm])\n#con_layer=Flatten()(con_layer)\n#con_layer=LSTM(128, return_sequences=True)(con_layer)\n#x = LSTM(512, return_sequences=False)(x)\ncon_out = Dense(128,activation='relu')(con_layer)\n#con_out = Dropout(0.5)(con_out)\n#con_out = Dense(1024,activation='sigmoid')(con_out)\ncon_out = Dropout(0.5)(con_layer)\ncon_out = Dense(3,activation='softmax')(con_out)\n#out = Dense(3,activation='softmax')(con_layer)\n\n# Defining model input and output\ncom_model = tf.keras.Model(inputs = [rmodel.input,modelse.input], outputs=con_out)\n#com_model.summary()\n# Using Stochastic gradient descent with optimizer\n# Compile the model\ncom_model.compile(\n    optimizer =tf.keras.optimizers.Adam(learning_rate=0.00001),# optimizer,#\n    loss = 'categorical_crossentropy',#loss\n    metrics =['accuracy'])# metric)#Max length 55 with ctext with class name with joint label\ncom_history = com_model.fit(\n    x =[vx_train,jx_train],\n    y = y_train,#to_categorical(train_df.tlabel),\n    #validation_data = ([vx_test,\n    #{'input_ids':tx_test['input_ids'],'attention_mask':tx_test['attention_mask']}],y_test #to_categorical(test_df.tlabel)\n   # ),\n validation_split = 0.1,\n  epochs=30,\n    batch_size=32\n)\n\n#text classification without english stop word and max word length 80\ncpredicted = com_model.predict([vx_test,jx_test])\ncom_score=com_model.evaluate([vx_test,jx_test],y_test)\nprint('Test Loss:', com_score[0])\nprint('Test accuracy:', com_score[1])\ncypredicted = np.argmax(cpredicted,axis=1)\n#y_predicted = np.where(predicted>0.5,1,0)\nfrom sklearn.metrics import classification_report\nprint(classification_report(test_df.jlabel,cypredicted))\n#accuracy vs loss function\ns, (at, al) = plt.subplots(2,1)\nat.plot(com_history.history['accuracy'], c= 'b')\nat.plot(com_history.history['val_accuracy'], c='r')\nat.set_title('model accuracy')\nat.set_ylabel('accuracy')\nat.set_xlabel('epoch')\nat.legend(['Res_bilstm_train', 'Res_bilstm_val'], loc='upper left')\nal.plot(com_history.history['loss'], c='m')\nal.plot(com_history.history['val_loss'], c='c')\nal.set_title('model loss')\nal.set_ylabel('loss')\nal.set_xlabel('epoch')\nal.legend(['train', 'val'], loc = 'upper left')\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jsequence_input = Input(shape=(max_len,), dtype='int32')\njembedding_sequences = jembedding_layer(jsequence_input)\nx = SpatialDropout1D(0.2)(jembedding_sequences)\n#x = Conv1D(128, 5, kernel_regularizer=tf.keras.regularizers.l2(0.001),activation='relu')(x)\nx = Conv1D(64, 5,activation='relu')(x)\n#x=Conv1D(64, 5,padding='same', kernel_initializer='he_uniform')(x)\n#x=Conv1D(32, 5,padding='same', kernel_initializer='he_uniform')(x)\n#cn = Dense(1024, activation='tanh')(x)\n#x=Conv1D(32, 5,padding='same',activation='relu')(x)\nx= GlobalMaxPool1D()(x)\n\n#x = Dropout(0.5)(x)\n#x = Dense(10, activation='relu')(x)\noutputs = Dense(3, kernel_regularizer=tf.keras.regularizers.l2(0.01),activation='softmax')(x)\n#outputs = Dense(3,activation='softmax')(x)\nmodelse= tf.keras.Model(jsequence_input, outputs)","metadata":{"execution":{"iopub.status.busy":"2024-02-12T23:11:13.602969Z","iopub.execute_input":"2024-02-12T23:11:13.603942Z","iopub.status.idle":"2024-02-12T23:11:13.645858Z","shell.execute_reply.started":"2024-02-12T23:11:13.603900Z","shell.execute_reply":"2024-02-12T23:11:13.644810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate convolutional base mode for whole image\nimage_input = Input(shape=(224, 224, 3), name='image')\nresnet = ResNet50(weights='imagenet',\n                  include_top=False,\n                  input_shape=(224, 224, 3))#(image_input)\nfor layer in resnet.layers:\n    layer.trainable=False\nres_layer=Flatten()(resnet.output)\nres_layer = Dense(256, activation='relu')(res_layer)#(resnet.output)\n#res_layer = Flatten()(res_layer) #(vgg16)#\nres_layer_32=Dense(32, activation='relu')(res_layer)\nimg_model=Dropout(0.5)(res_layer_32)\nvprediction = Dense(3, activation='softmax')(img_model)\nrmodel = tf.keras.Model(inputs=resnet.input, outputs=vprediction)\n#rmodel.summary()\n#img_model.summary()\n#rmodel.compile(loss='categorical_crossentropy',\n#                    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n #                   metrics=metric)#['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#when image and text data for cross attention\n#example\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Dot, Softmax, Multiply, Concatenate, Conv2D, MaxPooling2D, Flatten\n\ndef cross_attention(query, key, value):\n    # compute the attention scores between query and key\n    scores = Dot(axes=-1)([query, key])\n    scores = Softmax(axis=-1)(scores)\n\n    # apply the attention scores to the value sequence\n    context = Dot(axes=1)([scores, value])\n    context = tf.keras.layers.Reshape((1, -1))(context )\n    #context=Flatten()(context)\n    # concatenate the query and context vectors\n    output = Concatenate()([query, context])\n    return output\nrca_scene = tf.keras.layers.Reshape((1, -1))(ca_scene)\nrres_layer = tf.keras.layers.Reshape((1, -1))(res_layer)\n\nat2_context = cross_attention(rca_scene, rres_layer, rres_layer)\n#at2_context = cross_attention(ca_scene, res_layer, res_layer)\nat2_context=Flatten()(at2_context)\nat2 = Dense(256, activation='relu')(at2_context)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T19:16:01.787832Z","iopub.execute_input":"2024-02-20T19:16:01.788738Z","iopub.status.idle":"2024-02-20T19:16:01.840490Z","shell.execute_reply.started":"2024-02-20T19:16:01.788697Z","shell.execute_reply":"2024-02-20T19:16:01.839687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#new\n#when image and text data for cross attention\n#example\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Dot, Softmax, Multiply, Concatenate, Conv2D, MaxPooling2D, Flatten\n\ndef cross_attention(query, key, value):\n    # compute the attention scores between query and key\n    scores = Dot(axes=-1)([query, key])\n    scores = Softmax(axis=-1)(scores)\n\n    # apply the attention scores to the value sequence\n    context = Dot(axes=1)([scores, value])\n    context = tf.keras.layers.Reshape((1, -1))(context )\n    #context=Flatten()(context)\n    # concatenate the query and context vectors\n    output = Concatenate()([query, context])\n    return output\ntcl = tf.keras.layers.Reshape((1, -1))(tcl)\nres_layer = tf.keras.layers.Reshape((1, -1))( res_layer)\n\nat2_context = cross_attention(tcl, res_layer, res_layer)\nat2_context=Flatten()(at2_context)\nat2 = Dense(256, activation='relu')(at2_context)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T19:09:03.882954Z","iopub.execute_input":"2024-02-20T19:09:03.883414Z","iopub.status.idle":"2024-02-20T19:09:03.893337Z","shell.execute_reply.started":"2024-02-20T19:09:03.883377Z","shell.execute_reply":"2024-02-20T19:09:03.891513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fat1=Flatten()(at1)\n#fres_layer=Flatten()(res_layer)\n#fat2=Flatten()(at2)\n#fca_scene=Flatten()(ca_scene)\n#ftcl=Flatten()(tcl)\n#fscene=Flatten()(scene)\nfusion2 = tf.keras.layers.Concatenate()([fat2,fat1,fscene,fca_scene,fres_layer])\n#fusion2 = tf.keras.layers.Concatenate()([tcl,scene,ca_scene])\nfusion2 = Dense(256,activation='relu')(fusion2)#(att_input_layer)\nfusion2 = Dropout(0.5)(fusion2)\nfusion2_out = Dense(3,activation='softmax')(fusion2)\n# Defining model input and output\nf2model = tf.keras.Model(inputs = [rmodel.input,atclass_input,scene_input,ca_scene_input], outputs=fusion2_out)\n# Compile the model\nf2model.compile(\n    optimizer =tf.keras.optimizers.Adam(learning_rate=0.0005),# optimizer,#image_input\n    loss = 'categorical_crossentropy',#loss\n    metrics =['accuracy'])\n# metric)#Max length 55 with ctext with class name with joint label\nf2history = f2model.fit(\n    x =[vx_train,ax_train,sx_train,jx_train],\n    y = y_train,#to_categorical(train_df.tlabel),\n    #validation_data = ([vx_test,\n    #{'input_ids':tx_test['input_ids'],'attention_mask':tx_test['attention_mask']}],y_test #to_categorical(test_df.tlabel)\n   # ),\n validation_split = 0.1,\n  epochs=10,\n    batch_size=32\n)\n#text classification without english stop word and max word length 80\nf2predicted = f2model.predict([vx_test,ax_test,sx_test,jx_test])\n#com_score=f2model.evaluate([ax_test,sx_test,jx_test],y_test)\n#print('Test Loss:', com_score[0])\n#print('Test accuracy:', com_score[1])\nypredicted = np.argmax(f2predicted,axis=1)\n#y_predicted = np.where(predicted>0.5,1,0)\nfrom sklearn.metrics import classification_report\nprint(classification_report(test_df.ilabel,ypredicted,digits=4))\n\n#accuracy vs loss function\ns, (at, al) = plt.subplots(2,1)\nat.plot(f2history.history['accuracy'], c= 'b')\nat.plot(f2history.history['val_accuracy'], c='r')\nat.set_title('model accuracy')\nat.set_ylabel('accuracy')\nat.set_xlabel('epoch')\nat.legend(['Res_bilstm_train', 'Res_bilstm_val'], loc='upper left')\n\nal.plot(f2history.history['loss'], c='m')\nal.plot(f2history.history['val_loss'], c='c')\nal.set_title('model loss')\nal.set_ylabel('loss')\nal.set_xlabel('epoch')\nal.legend(['train', 'val'], loc = 'upper left')\nbal_acc=balanced_accuracy_score(test_df.ilabel,ypredicted)\nrecall=recall_score(test_df.ilabel,ypredicted,average='weighted')\nprecision=precision_score(test_df.ilabel,ypredicted,average='weighted')\nf1=f1_score(test_df.ilabel,ypredicted,average='weighted')\nprint(acc,bal_acc,recall,precision,f1)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T19:59:34.739637Z","iopub.execute_input":"2024-02-20T19:59:34.740018Z","iopub.status.idle":"2024-02-20T20:01:04.435404Z","shell.execute_reply.started":"2024-02-20T19:59:34.739979Z","shell.execute_reply":"2024-02-20T20:01:04.434331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenating the last layer of two model global+bert\nres_layer=Flatten()(res_layer)\ncn=Flatten()(x)\ncon_layer = concatenate([res_layer,cn])\n#con_layer=Flatten()(con_layer)\n#con_layer=LSTM(128, return_sequences=True)(con_layer)\n#x = LSTM(512, return_sequences=False)(x)\ncon_layer = Dropout(0.5)(con_layer)\ncon_out = Dense(128,activation='relu')(con_layer)\ncon_out = Dropout(0.5)(con_out)\n#con_out = Dense(1024,activation='sigmoid')(con_out)\n\ncon_out = Dense(3,activation='softmax')(con_out)\n#out = Dense(3,activation='softmax')(con_layer)\n\n# Defining model input and output\ncom_model = tf.keras.Model(inputs = [rmodel.input,modelse.input], outputs=con_out)\n#com_model.summary()\n# Using Stochastic gradient descent with optimizer\n# Compile the model\ncom_model.compile(\n    optimizer =tf.keras.optimizers.Adam(learning_rate=0.0005),# optimizer,#\n    loss = 'categorical_crossentropy',#loss\n    metrics =['accuracy'])# metric)#Max length 55 with ctext with class name with joint label\ncom_history = com_model.fit(\n    x =[vx_train,jx_train],\n    y = y_train,#to_categorical(train_df.tlabel),\n    #validation_data = ([vx_test,\n    #{'input_ids':tx_test['input_ids'],'attention_mask':tx_test['attention_mask']}],y_test #to_categorical(test_df.tlabel)\n   # ),\n validation_split = 0.1,\n  epochs=10,\n    batch_size=32\n)\n\n#text classification without english stop word and max word length 80\ncpredicted = com_model.predict([vx_test,jx_test])\n#com_score=com_model.evaluate([vx_test,jx_test],y_test)\n#print('Test Loss:', com_score[0])\n#print('Test accuracy:', com_score[1])\ncypredicted = np.argmax(cpredicted,axis=1)\n#y_predicted = np.where(predicted>0.5,1,0)\nfrom sklearn.metrics import classification_report\nprint(classification_report(test_df.ilabel,cypredicted))\n#accuracy vs loss function\ns, (at, al) = plt.subplots(2,1)\nat.plot(com_history.history['accuracy'], c= 'b')\nat.plot(com_history.history['val_accuracy'], c='r')\nat.set_title('model accuracy')\nat.set_ylabel('accuracy')\nat.set_xlabel('epoch')\nat.legend(['Res_bilstm_train', 'Res_bilstm_val'], loc='upper left')\n\nal.plot(com_history.history['loss'], c='m')\nal.plot(com_history.history['val_loss'], c='c')\nal.set_title('model loss')\nal.set_ylabel('loss')\nal.set_xlabel('epoch')\nal.legend(['train', 'val'], loc = 'upper left')\n\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\n\nprint(classification_report(test_df.ilabel,cypredicted,digits=4))\nacc=accuracy_score(test_df.ilabel,cypredicted)\nbal_acc=balanced_accuracy_score(test_df.ilabel,cypredicted)\nrecall=recall_score(test_df.ilabel,cypredicted,average='weighted')\nprecision=precision_score(test_df.ilabel,cypredicted,average='weighted')\nf1=f1_score(test_df.ilabel,cypredicted,average='weighted')\nprint(acc,bal_acc,recall,precision,f1)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T20:20:41.736525Z","iopub.execute_input":"2024-02-13T20:20:41.736928Z","iopub.status.idle":"2024-02-13T20:21:28.486578Z","shell.execute_reply.started":"2024-02-13T20:20:41.736893Z","shell.execute_reply":"2024-02-13T20:21:28.485628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cypredicted = np.argmax(cpredicted,axis=1)\n#y_predicted = np.where(predicted>0.5,1,0)\nfrom sklearn.metrics import classification_report\nprint(classification_report(test_df.ilabel,cypredicted))\n#accuracy vs loss function\ns, (at, al) = plt.subplots(2,1)\nat.plot(com_history.history['accuracy'], c= 'b')\nat.plot(com_history.history['val_accuracy'], c='r')\nat.set_title('model accuracy')\nat.set_ylabel('accuracy')\nat.set_xlabel('epoch')\nat.legend(['Res_bilstm_train', 'Res_bilstm_val'], loc='upper left')\n\nal.plot(com_history.history['loss'], c='m')\nal.plot(com_history.history['val_loss'], c='c')\nal.set_title('model loss')\nal.set_ylabel('loss')\nal.set_xlabel('epoch')\nal.legend(['train', 'val'], loc = 'upper left')\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\n\nprint(classification_report(test_df.ilabel,cypredicted,digits=4))\nacc=accuracy_score(test_df.ilabel,cypredicted)\nbal_acc=balanced_accuracy_score(test_df.ilabel,cypredicted)\nrecall=recall_score(test_df.ilabel,cypredicted,average='weighted')\nprecision=precision_score(test_df.ilabel,cypredicted,average='weighted')\nf1=f1_score(test_df.ilabel,cypredicted,average='weighted')\nprint(acc,bal_acc,recall,precision,f1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''# Concatenating the last layer of two model\nres_layer=Flatten()(res_layer)\ncon_layer = concatenate([v16_layer,res_layer])\ncon_layer=Flatten()(con_layer)\n#con_layer=LSTM(128, return_sequences=True)(con_layer)\n#x = LSTM(512, return_sequences=False)(x)\n#con_out = Dense(2048,activation='relu')(con_layer)\n#con_out = Dropout(0.5)(con_out)\n#con_out = Dense(1024,activation='sigmoid')(con_out)\ncon_out = Dropout(0.5)(con_layer)\ncon_out = Dense(3,activation='sigmoid')(con_out)\n#out = Dense(3,activation='softmax')(con_layer)\n\n# Defining model input and output\nvcom_model = tf.keras.Model(inputs = [wmodel.input,rmodel.input], outputs=con_out)\n#com_model.summary()\n# Using Stochastic gradient descent with optimizer\n# Compile the model resnet+ object+text\n# Compile the model\nvcom_model.compile(\n    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0008),#optimizer,#\n    loss = 'categorical_crossentropy',#loss\n    metrics = metric)#Max length 55 with ctext with class name with joint label\nvcom_history = vcom_model.fit(\n    x =[vx_train,ox_train],\n    y = vy_train,#to_categorical(train_df.tlabel),\n    validation_data = ([vx_test,ox_test],vy_test #to_categorical(test_df.tlabel)\n    ),\n  epochs=10,\n    batch_size=64\n)\n\n#text classification without english stop word and max word length 80\ncpredicted = vcom_model.predict([vx_test,ox_test])\ncom_score=vcom_model.evaluate([vx_test,ox_test],vy_test)\nprint('Test Loss:', com_score[0])\nprint('Test accuracy:', com_score[1])\ncypredicted = np.argmax(cpredicted,axis=1)\n#y_predicted = np.where(predicted>0.5,1,0)\nfrom sklearn.metrics import classification_report\nprint(classification_report(test_df.vlabel,cypredicted))'''","metadata":{"papermill":{"duration":0.344883,"end_time":"2022-09-18T13:45:32.277565","exception":false,"start_time":"2022-09-18T13:45:31.932682","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.437069,"end_time":"2022-09-18T13:48:53.745457","exception":false,"start_time":"2022-09-18T13:48:53.308388","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''# Instantiate convolutional base\nimage_input = Input(shape=(224, 224, 3), name='image1')\nvgg19 = VGG19(weights='imagenet',\n                  include_top=False,\n                  input_shape=(224, 224, 3))#(image_input)\noresnet = ResNet50(weights='imagenet',\n                  include_top=False,\n                  input_shape=(128, 128, 3))#(image_input)\n#global_model = layers.Flatten()(vgg16) \n#x = layers.Dense(32, activation='relu')(x)\nfor layer in vgg19.layers:\n    layer.trainable = False\nv19_layer=Flatten()(vgg19.output)    \n#v19_layer=Dense(512,activation='relu')(vgg19.output)\n#g_layer=Dropout(0.5)(v19_layer)\n#g_layer=Dense(1024,activation='relu')(vgg19.output)\ng_layer=Dropout(0.5)(v19_layer)\ng_out=Dense(3,activation='sigmoid')(g_layer)\nv19_model = tf.keras.Model(inputs=vgg19.input, outputs=g_out)\n#img_model.compile(loss='categorical_crossentropy',\n                   # optimizer=tf.keras.optimizers.Adam(learning_rate=0.00008),\n                    #metrics=['accuracy'global_model.summary()'''","metadata":{"papermill":{"duration":0.899781,"end_time":"2022-09-18T13:53:50.626687","exception":false,"start_time":"2022-09-18T13:53:49.726906","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nv19_model.compile(loss='categorical_crossentropy',\n                    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n                    metrics=metric)#['accuracy'])\nhistoryr = v19_model.fit(vx_train,vy_train, \n                   # steps_per_epoch=STEP_SIZE_TRAIN, \n                    epochs =10, verbose=1, \n                    validation_data = (vx_test,vy_test))\n                    #validation_steps = STEP_SIZE_TEST)\n#when whole dataset is divided training,and testing\nscore = v19_model.evaluate(vx_test,vy_test)\nprint('Test Loss:', score[0])\nprint('Test accuracy:', score[1])\n#duration = datetime.now() - start\n#print(\"Training completed in time: \", duration)\nprint('Test Loss:', score[0])\nprint('Test accuracy:', score[1])\nimport matplotlib.pyplot as plt\nplt.plot(historyr.history['accuracy'])\nplt.plot(historyr.history['val_accuracy'])\nplt.plot(historyr.history['loss'])\nplt.plot(historyr.history['val_loss'])\nplt.title('model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Accuracy','Validation Accuracy','loss','Validation Loss'])\nplt.show()\n#imscore = img_model.evaluate(vx_test,vy_test, verbose=4)\nprint('Test Loss:', score[0])\nprint('Test accuracy:', score[1])\n#METRICS\n\nimpredicted = v19_model.predict(vx_test)\nimypredicted = np.argmax(impredicted,axis=1)\n#y_predicted = np.where(predicted>0.5,1,0)\nfrom sklearn.metrics import classification_report\nprint(classification_report(test_df.vlabel,imypredicted))'''","metadata":{"papermill":{"duration":563.498943,"end_time":"2022-09-18T14:03:14.654341","exception":false,"start_time":"2022-09-18T13:53:51.155398","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_kg_hide-output":true,"papermill":{"duration":0.563014,"end_time":"2022-09-18T14:03:15.775480","exception":false,"start_time":"2022-09-18T14:03:15.212466","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''# Concatenating the last layer of two model C vgg19+ object+text\n#v19_layer=Flatten()(vgg19.output) \n#v19_layer=Flatten()(v19_layer)\ncon_layer = concatenate([v19_layer,bout])\ncon_layer=Flatten()(con_layer)\n#con_layer=LSTM(128, return_sequences=True)(con_layer)\n#x = LSTM(512, return_sequences=False)(Compile the model vgg19+ object+textx)\n#con_out = Dense(2048,activation='relu')(con_layer)\n#con_out = Dropout(0.5)(con_out)\n#con_out = Dense(1024,activation='sigmoid')(con_out)\ncon_out = Dropout(0.5)(con_layer)\ncon_out = Dense(3,activation='sigmoid')(con_out)\n#out = Dense(3,activation='softmax')(con_layer)\n\n# Defining model input and output\ncom_model = tf.keras.Model(inputs = [v19_model.input,tbmodel.input], outputs=con_out)\n#com_model.summary()\n# Using Stochastic gradient descent with optimizer\n# Compile the model vgg19+ object+text\n# Compile the model\ncom_model.compile(\n    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001),#optimizer,#\n    loss = 'categorical_crossentropy',#loss\n    metrics = metric)#Max length 55 with ctext with class name with joint label\ncom_history = com_model.fit(\n    x =[vx_train,{'input_ids':tx_train['input_ids'],'attention_mask':tx_train['attention_mask']}],\n    y = jy_train,#to_categorical(train_df.tlabel),\n    validation_data = ([vx_test,\n    {'input_ids':tx_test['input_ids'],'attention_mask':tx_test['attention_mask']}],jy_test #to_categorical(test_df.tlabel)\n    ),\n  epochs=5,\n    batch_size=20\n)\n\n#text classification without english stop word and max word length 80\ncpredicted = com_model.predict([vx_test,{'input_ids':tx_test['input_ids'],'attention_mask':tx_test['attention_mask']}])\ncom_score=com_model.evaluate([vx_test,{'input_ids':tx_test['input_ids'],'attention_mask':tx_test['attention_mask']}],jy_test)\nprint('Test Loss:', com_score[0])\nprint('Test accuracy:', com_score[1])\ncypredicted = np.argmax(cpredicted,axis=1)\n#y_predicted = np.where(predicted>0.5,1,0)\nfrom sklearn.metrics import classification_report\nprint(classification_report(test_df.jlabel,cypredicted))'''","metadata":{"papermill":{"duration":361.588872,"end_time":"2022-09-18T14:09:17.959756","exception":false,"start_time":"2022-09-18T14:03:16.370884","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.631774,"end_time":"2022-09-18T14:09:19.186796","exception":false,"start_time":"2022-09-18T14:09:18.555022","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''#Combine  global-v16 for local-resnet, and bert\n#img_layer=Flatten()(img_layer)\n#obj_layer=Flatten()(obj_layer)\n#vit_layer=Flatten()(vit_layer)\n#img_layer=Flatten()img_layer\n# Concatenating the last layer of two model \ncon_layer = concatenate([v16_layer,ores_layer,bout])\n#con_layer=Flatten()(con_layer)\n#con_layer=LSTM(128, return_sequences=True)(con_layer\n#x = LSTM(512, return_sequences=False)(x)\n#con_out = Dense(1024,activation='relu')(con_layer)\n#con_out = Dropout(0.5)(con_out)\n#con_out = Dense(1024,activation='relu')(con_layer)\n#con_out = Dropout(0.2)(con_out)\n#con_out = Dense(1024,activation='relu')(con_layer)\n#con_out = Dropout(0.2)(con_out)\ncon_out = Dense(256,activation='relu')(con_layer)\ncon_out = Dropout(0.2)(con_out)\ncon_out = Dense(3,activation='softmax')(con_out)\n#out = Dense(3,activation='softmax')(con_layer)\n\n# Defining model input and output\ncom_model = tf.keras.Model(inputs = [wmodel.input,ormodel.input, tbmodel.input], outputs=con_out)\n#com_model.summary()\n# Using Stochastic gradient descent with optimizer\n# Compile the model\ncom_model.compile(\n    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001),# optimizer,\n    loss = 'categorical_crossentropy',#loss, \n    metrics = metric)\n#Max length 55 with ctext with class name with joint label\ncom_history = com_model.fit(\n    x =[vx_train,ox_train,{'input_ids':tx_train['input_ids'],'attention_mask':tx_train['attention_mask']}],\n    y = jy_train,#to_categorical(train_df.tlabel),\n    validation_data = ([vx_test,ox_test,\n    {'input_ids':tx_test['input_ids'],'attention_mask':tx_test['attention_mask']}],jy_test #to_categorical(test_df.tlabel)\n    ),\n  epochs=10,\n    batch_size=8\n)\n\n#text classification without english stop word and max word length 80\ncpredicted = com_model.predict([vx_test,ox_test,\n    {'input_ids':tx_test['input_ids'],'attention_mask':tx_test['attention_mask']}])\ncom_score=com_model.evaluate([vx_test,ox_test,\n    {'input_ids':tx_test['input_ids'],'attention_mask':tx_test['attention_mask']}],jy_test)\nprint('Test Loss:', com_score[0])\nprint('Test accuracy:', com_score[1])\ncypredicted = np.argmax(cpredicted,axis=1)\n#y_predicted = np.where(predicted>0.5,1,0)\nfrom sklearn.metrics import classification_report\nprint(classification_report(test_df.jlabel,cypredicted))'''","metadata":{"papermill":{"duration":585.167375,"end_time":"2022-09-18T14:38:52.723816","exception":false,"start_time":"2022-09-18T14:29:07.556441","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.840716,"end_time":"2022-09-18T14:50:53.851076","exception":false,"start_time":"2022-09-18T14:50:53.010360","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}